#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble


\AtBeginDocument{
  \def\labelitemii{\ding{71}}
  \def\labelitemiii{\ding{111}}
  \def\labelitemiv{\(\vartriangleright\)}
}



\usepackage{babel}
\addto\shorthandsspanish{\spanishdeactivate{~<>}}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding iso8859-15
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle plain
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
State of the Art
\begin_inset CommandInset label
LatexCommand label
name "chap:State-of-the art"

\end_inset


\end_layout

\begin_layout Standard
As explained in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "cha:Introduction"

\end_inset

 the process of analysis of a common crowed space will embrace many algorithms
 from different Computer Vision disciplines.
 In the scope of our work it will be worthy to analyze deeply state of the
 art pedestrian detection approaches and how to improve them as it will
 be a pivotal field for our goals.
 In addition contextual information an its main algorithms will be also
 important an the actual literature should be analyzed.
 Both of these disciplines will be used in a multi-camera setup and so,
 scenarios working with this configuration will be defined.
\end_layout

\begin_layout Standard
Throughout this Chapter we will summarize the actual and most used algorithms
 in the different mentioned categories.
\end_layout

\begin_layout Section
Pedestrian detection and crowd patrons
\begin_inset CommandInset label
LatexCommand label
name "sec:Pedestrian-detection-and crowd patrons"

\end_inset


\end_layout

\begin_layout Standard
Pedestrian detection has been a major issue in computer vision during the
 past few years due to its potential uses for all sort of modern applications.
 Its main objective is to detect and identify a potential object as a person
 and by the end, obtain its position in the scene.
\end_layout

\begin_layout Standard
Nowadays, it is consider a half-solved problem.
 Although there are many excellent pedestrian detectors in the literature
 the is not a detector that performs with perfect accuracy.
 This is one of the reasons why it keeps being one of the most researched
 area in the computer vision field.
\end_layout

\begin_layout Standard
The difficulty of pedestrian detector lies in the large amount of available
 datasets with different video and people characteristics such as people
 occlusions, poses, scales, illumination ranges.
\end_layout

\begin_layout Subsection
People detection approaches
\end_layout

\begin_layout Standard
Main pedestrian detections that have been used so far can be divided in
 many categories and from many points of views.
 One possible easy classification depending on the used descriptor may start
 with the ones that use Histograms of Oriented Gradients like 
\begin_inset CommandInset citation
LatexCommand cite
key "dalal2005histograms"

\end_inset

 in combination with linear Support Vectors Machine in order to describe
 pedestrian shape with gradients to create a model and classify possible
 candidates.
 Secondly, Discriminative Part Models as 
\begin_inset CommandInset citation
LatexCommand cite
key "felzenszwalb2008discriminatively"

\end_inset

 that will divide the human body into different parts (head, trunk, legs...)
 and search for them into the image for a later combination into a single
 person detection, or finally Aggregate Channel Features 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset

 which will use a combination of different channels such as normalized gradient
 magnitude, histogram of oriented gradients (6 channels), and LUV color
 channels in order to achieve the final detection.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "garcia2015people"

\end_inset

 an extensive evaluation of the state of the art of people detection is
 given.
 Here algorithms are characterize and differentiate ones from the others
 depending on the object detection approach and the person model used.
 Object detection will be the extraction of the possible candidates to be
 a person from the scene.
 Main algorithms usually will use:
\end_layout

\begin_layout Itemize
Sliding window: The so called exhaustive search will lead to an efficient
 classifier to test, in search of pedestrian, every possible image window.
 Parameters such as window size, or overlapping are common tuning values
 that will increase or decrease the performance of the detector.
 Those methods usually need from 
\begin_inset Formula $10^{4}$
\end_inset

 to 
\begin_inset Formula $10^{5}$
\end_inset

 windows per image and this number will grow exponentially for multi-scale
 detection.
 If the complexity of the core classifier is increased in every window testing,
 the computational time will end up being not affordable.
\end_layout

\begin_layout Itemize
Segmentation: This approach will introduce some sort of segmentation as
 a preliminary step for the pedestrian detection.
 Algorithms such as background subtraction will lead to some moving parts
 of the image that will generate interest objects from moving patterns.
 Others such as color segmentation are based on color skin to restrict the
 future people search only to those objects that fulfill the color condition.
 By all means, segmentation will directly produce those scene candidates
 to be a person and so the computational time is highly decreased.
\end_layout

\begin_layout Itemize
Segmentation + Exhaustive search: The final approach will be the combination
 of both previously analyzed techniques.
 In this case the previous step of segmentation will not lead to final candidate
s objects but to an area that could contain some candidates objects.
 After segmentation process and sliding window technique is perform over
 the reduce scene area and so the final candidates are extracted.
 In this case improvements from both approaches are used as the computational
 cost of the exhaustive search (that is the main drawback) is reduce by
 the use of some sort of segmentation.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Edge-Detector-Example"

\end_inset

 we can observe a diagram example of the Edge detector that uses the segmentatio
n + exhaustive search approach.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/Edge Pedestrian Detector Example.png
	lyxscale 30
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Edge Detector Example Diagram
\begin_inset CommandInset label
LatexCommand label
name "fig:Edge-Detector-Example"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Moving to person model, or in other words the set of characteristics that
 will be able to discriminate between people and any other object in the
 scene, 
\begin_inset CommandInset citation
LatexCommand cite
key "garcia2015people"

\end_inset

 differentiates three different classification groups:
\end_layout

\begin_layout Itemize
Based on appearance: Most of the available detectors nowadays use appearance
 information to define the person model and so, to discriminate between
 person and the rest of the scene.
 In this group of person model one can differentiate two approaches to describe
 the shape of a person.
\end_layout

\begin_deeper
\begin_layout Standard
Holistic: With this models people will be defined from the easiest point
 of view when a person is a region or a shape
\end_layout

\begin_layout Standard
Part-based: These models however, use more complex model where people are
 defined as a combination of multiple shapes or regions from its body.
\end_layout

\begin_layout Standard
Here one can find those that use silhouettes to classify people, either
 from an holistic or part-based point of view, or color distribution in
 people, however the most used approaches as said at the beginning of the
 section, use Histograms of Oriented Gradients like 
\begin_inset CommandInset citation
LatexCommand cite
key "dalal2005histograms"

\end_inset

, Haar-like features 
\begin_inset CommandInset citation
LatexCommand cite
key "alonso2007combination"

\end_inset

, or combination of multiple features, aggregate channel features: HOG,
 gradient and color (ACF) 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Based on motion: As said before many of the detectors usually are based
 on appearance, however, human appearance is likely to change due to environment
al factors such as light conditions, cloths, camera settings.
 In addition, people variability in terms of height, weigh and poses will
 make appearance much likely to vary.
 Due to this, some approaches try to get rid of these factors and detect
 pedestrians using only its motion information.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "cutler2000robust"

\end_inset

 detections are based on periodic motion analysis.
 The algorithm performs motion segmentation and tracking to later on compute
 the self-similarity between objects.
\end_layout

\begin_layout Itemize
Based on appearance + motion: Although the main used algorithms are based
 on appearance there are some such as 
\begin_inset CommandInset citation
LatexCommand cite
key "giebel2004bayesian,okuma2004boosted"

\end_inset

 that will merge both appearance and motion in order to improve results.
 Most of these algorithms combine people detection and tracking, and so,
 they have been design to improve people tracking over video sequences.
\end_layout

\begin_layout Standard
In the Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Pedestrian-detection-performance"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "garcia2015people"

\end_inset

 propose some results from the State of the Art detectors depending on different
 datasets.
 The selected algorithm ranking for this evaluation is the average AUC.
 This metric will measure the area under a Recall Over Precision curve which
 means that an area of 1 represents a perfect test; an area of .5 represents
 a worthless test
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength
\backslash
extrarowheight{7pt}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="8">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Video
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HOG
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ISM
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Fusion
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Edge
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
DTDP
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ACF
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Faster-RCNN
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
89.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
71.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
34.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
84.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
96.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.7
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
69.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
82.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
92.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
90.2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
77.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
77.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
98.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
55.6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
75.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
71.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
68.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
68.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
82.9
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
33.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
33.9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
37.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Average AUC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Pedestrian Detection Performance
\begin_inset CommandInset label
LatexCommand label
name "tab:Pedestrian-detection-performance"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, last years a new sort of detectors have start to be use detectors
 based on deep Convolutional Networks have improve notably the accuracy
 of all the previous algorithms.
 Examples such as ImageNet 
\begin_inset CommandInset citation
LatexCommand cite
key "krizhevsky2012imagenet"

\end_inset

 for image classification and Fast R-CNN 
\begin_inset CommandInset citation
LatexCommand cite
key "girshick2015fast"

\end_inset

 for object detection expose that deep convolutional networks outperform
 the previous mentioned algorithms.
 This fact is clearly presented in the mentioned Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Pedestrian-detection-performance"

\end_inset

.
\end_layout

\begin_layout Subsection
Available Datasets
\end_layout

\begin_layout Standard
As said at the beginning of the section pedestrian approaches will perform
 better or worse depending on the dataset characteristics that have been
 use for learning.
 Due to the high number of available dataset no detector can outperform
 on all of them.
 During this section main used datasets will be analyzed:
\end_layout

\begin_layout Itemize
Caltech 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2009pedestrian"

\end_inset

, which consists on video taken from a vehicle diving through regular traffic
 in an urban environment.
\end_layout

\begin_layout Itemize
ETHZ 
\begin_inset CommandInset citation
LatexCommand cite
key "ess2007depth"

\end_inset

.
 It consists on recordings using a pair of AVT Marlins F033C mounted on
 a chariot which moves through pedestrian paths.
\end_layout

\begin_layout Itemize
TUD 
\begin_inset CommandInset citation
LatexCommand cite
key "wojek2009multi"

\end_inset

.
 In this case, the recording has been done by a static camera in a crossing
 campus scene.
\end_layout

\begin_layout Itemize
INRIA 
\begin_inset CommandInset citation
LatexCommand cite
key "dalal2005histograms"

\end_inset

 which collects precise people images both static and moving.
 In this case, images fully represent people without taking into account
 the environment.
\end_layout

\begin_layout Standard
As a result of this large amount of training and testing data, some detectors
 perform better in terms of precision and time consumption than others depending
 on the specific situations.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Image-summary-from datasets"

\end_inset

 one can observe some examples from the images that can be obtained from
 the different mentioned datasets.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/Caltech Dataset.jpg
	lyxscale 80
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Caltech 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2009pedestrian"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/ETHZ Dataset.png
	lyxscale 30
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ETHZ 
\begin_inset CommandInset citation
LatexCommand cite
key "ess2007depth"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/TUD Campus Dataset.png
	lyxscale 30
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
TUD 
\begin_inset CommandInset citation
LatexCommand cite
key "wojek2009multi"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/Inria Dataset.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
INRIA 
\begin_inset CommandInset citation
LatexCommand cite
key "dalal2005histograms"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Image summary from all the analyzed datasets.
\begin_inset CommandInset label
LatexCommand label
name "fig:Image-summary-from datasets"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Object Proposals
\begin_inset CommandInset label
LatexCommand label
name "subsec:Object-Proposals"

\end_inset


\end_layout

\begin_layout Standard
Described detectors such as HOG, DPM or ACF utilized the well known 
\begin_inset Quotes eld
\end_inset

sliding window
\begin_inset Quotes erd
\end_inset

 algorithm, which means that an efficient classifier will test, in search
 of pedestrian, every possible image window.
 Parameters such as window size, or overlapping are common tuning values
 that will increase or decrease the performance of the detector.
 Those methods usually need from 
\begin_inset Formula $10^{4}$
\end_inset

 to 
\begin_inset Formula $10^{5}$
\end_inset

 windows per image and this number will grow exponentially for multi-scale
 detection.
 If the complexity of the core classifier is increased in every window testing,
 the computational time will end up being not affordable.
\end_layout

\begin_layout Standard
One of the most successful approach to overcame this time consumption problem
 without losing detection quality is by means of object proposals 
\begin_inset CommandInset citation
LatexCommand cite
key "hosang2016makes"

\end_inset

.
\end_layout

\begin_layout Standard
Object proposals will be such objects that could be fitted into the desired
 label classification.
 This object will be share common visual properties that will distinguish
 them from the background.
 In order words, this approaches will perform a complete search over an
 image to previously select potential pedestrian candidates.
\end_layout

\begin_layout Standard
If lower number of object proposals than sliding windows lead to a higher
 object recall, a speed-up will be achieve which means that more efforts
 can be focus on more sophisticated classifiers.
\end_layout

\begin_layout Standard
Both ImageNet and Fast R-CNN use detections proposals, which is one of the
 main reasons why they outperform previous algorithms.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "hosang2016makes"

\end_inset

 two set of proposal methods are analyzed:
\end_layout

\begin_layout Itemize
Grouping proposal methods.
 This methods attempt to generate multiple, and so, overlapping segment
 that are likely to correspond to objects.
 Usually this method generate region proposals by grouping super pixels,
 solving multiple graph cut problems, or directly using edge contours.
 One of the best examples for this methods is MCG 
\begin_inset CommandInset citation
LatexCommand cite
key "arbelaez2014multiscale"

\end_inset

.
\end_layout

\begin_layout Itemize
Window scoring proposal methods.
 An alternate approach is to score each candidate window according to the
 probability to contain an object.
 Best results for this methods are provided by EdgeBoxes 
\begin_inset CommandInset citation
LatexCommand cite
key "zitnick2014edge"

\end_inset

 which uses a coarse sliding window pattern which uses boundary estimation
 and a refinement step to improve performance.
\end_layout

\begin_layout Subsection
Research Directions
\end_layout

\begin_layout Standard
As said before this computer vision field is constantly in change and so
 some future work lines can be set.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2012pedestrian"

\end_inset

 some research directions are proposed that could be of interest in the
 scope of this work.
 
\end_layout

\begin_layout Enumerate
Context information should be added.
 Starting from the hypothesis that a person should be placed on the floor,
 the ground plane assumption can reduce errors if the detection for both
 the person and the floor are accurate.
 This could be achieve by extracting good contextual information from the
 scene which is one of the main objectives of the work.
\end_layout

\begin_layout Enumerate
Occlusion treatment.
 Usually pedestrians, due to other scene elements such as columns or even
 other pedestrians get occluded.
 When this happens performance degrades rapidly under even mild occlusion
 so, some improvements in this area will increase the performance in real
 life situations such as crowded spaces.
 
\end_layout

\begin_layout Subsection
Crowd Dynamics
\end_layout

\begin_layout Standard
Finally, there is many literature concerning pedestrian dynamics in the
 real world such as the Social Force Model (SFM) 
\begin_inset CommandInset citation
LatexCommand cite
key "mazzon2013multi"

\end_inset

 or people behavior in a built environment 
\begin_inset CommandInset citation
LatexCommand cite
key "turner2002encoding"

\end_inset

.
 The information coming from these algorithms will lead to predictions in
 people behavior and the paths they could follow which could help us to
 extract more accurate statistical data, refine people detections based
 on this previous behavior knowledge or even improve detection computational
 time searching pedestrian only in those paths they usually follow.
\end_layout

\begin_layout Section
Contextual Information
\end_layout

\begin_layout Standard
One can describe contextual information as the set of circumstances or facts
 that one can extract from a scene.
 Usually, this set of circumstances is a wealth of information that is not
 captured by the image but extracted by humans based on previously acquired
 knowledge.
 By taking a look to an outdoor image one can derive where the sky will
 be, what the weather conditions are, which time of the day...
 Also, by knowing the place the photo was taken, one can lead which objects
 will be more probable to appear in the scene.
\end_layout

\begin_layout Standard
When talking specifically about computer vision, contextual information
 will embrace for instance camera information (such as position, configuration,
 distance to an object, static or not), the set of objects that one could
 detect in the scene or how many views will be available.
\end_layout

\begin_layout Standard
All this set of information that is not extracted from the frame will provide
 a more general understanding about the scene and will help further algorithms.
 We can divide contextual information into two different levels, global
 and local contextual information, and also into two different categories
 offline, and online.
\end_layout

\begin_layout Subsection
Global
\end_layout

\begin_layout Standard
Global context will consider image detections from the image as a whole,
 this means that for instance an image will be consider as a kitchen to
 predict the presence of a stove.
 This kind of approaches are focused on psychology studies 
\begin_inset CommandInset citation
LatexCommand cite
key "navon1977forest,rensink1997see"

\end_inset

 that suggests that human perceptual processes work following a hierarchically
 organized process.
 Our perception system will go from a global structure towards a more detailed
 analysis.
\end_layout

\begin_layout Standard
Global context approaches aims to define a scene as an extra source of global
 information.
 The structure of a determinate scene image can be estimated by the mean
 of global image features.
\end_layout

\begin_layout Subsection
Local
\end_layout

\begin_layout Standard
On the contrary, local context will consider context information from the
 neighbor area of an object, e.g.
 a kitchen table will predict the presence of a spoon.
 The neighboring areas will be defined as a set of other objects, patches
 or even pixels.
 The aim is to try to correctly define the area that surrounds the object
 to precisely detect the desired object.
 
\end_layout

\begin_layout Subsection
Offline
\end_layout

\begin_layout Standard
Offline contextual information will be the set of circumstances that are
 computed and available before starting any kind of procedure.
 This information will be previously computed and so, it will lead to external
 image information that will added to any algorithm knowledge.
\end_layout

\begin_layout Subsection
Online
\end_layout

\begin_layout Standard
Online information, on the other hand, will be computed during procedures,
 and so it will not previously computed knowledge.
 Online extraction will have the computational time drawback as it will
 be considered as part of an algorithm.
\end_layout

\begin_layout Standard
If we now focus with the set of objects that one can observe from a scene,
 semantic segmentation techniques, which will be analyzed in the following
 section, aim to separate a scene into different observable objects.
\end_layout

\begin_layout Subsection
Semantic segmentation
\end_layout

\begin_layout Standard
Semantic information can often lead to low-level characteristics 
\begin_inset CommandInset citation
LatexCommand cite
key "manjunath2002introduction"

\end_inset

 such as global color or, on the other hand, it can describe an image by
 the position and status of relevant objects 
\begin_inset CommandInset citation
LatexCommand cite
key "oh2010content"

\end_inset

 such as cars according to its moving paths or, what is more important to
 our related work, concrete areas in a scene such as walls, corridors, walking
 paths, etc.
 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

.
\end_layout

\begin_layout Standard
In other words, semantic segmentation will have the goal to assign each
 pixel of an image a category label.
 If the prediction is accurate, it will provide complete semantic understanding
 from the scene (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Scene-parsing-on PSPNet"

\end_inset

) which means that one could have the position and label of some object.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/PSPNet Image.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Scene parsing on ADE20K 
\begin_inset CommandInset citation
LatexCommand cite
key "zhou2016semantic"

\end_inset

.
 Image from 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Scene-parsing-on PSPNet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cityscapes Dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "cordts2016cityscapes"

\end_inset

 is a large-scale dataset that contains stereo video sequences recorded
 in street scenes from among 50 different cities around the world.
 This dataset presents class annotations over pixels in more than 5000 frames.
 In the following table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Cityscapes-Dataset-Class Definitions"

\end_inset

 available dataset classes are presented.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength
\backslash
extrarowheight{7pt}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Category
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Classes
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Flat
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
road · sidewalk · parking · rail track
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Human
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
person · rider
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Vehicle
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
car · truck · bus · on rails · motorcycle · bicycle · caravan · trailer
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Construction
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
building · wall · fence · guard rail · bridge · tunnel
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
pole · pole group · traffic sign · traffic light
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nature
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vegetation · terrain
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sky
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
sky
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Void
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ground · dynamic · static
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cityscapes Dataset Class Definitions
\begin_inset CommandInset label
LatexCommand label
name "tab:Cityscapes-Dataset-Class Definitions"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "cordts2016cityscapes"

\end_inset

 offers a benchmark suite with an evaluation server.
 This makes easy that authors can upload semantic segmentation results in
 order to be able to compare their performance with the rest of tested algorithm
s.
 In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Cityscapes Dataset Challenge Results"

\end_inset

 we can observe the four most accurate algorithms in the scope of this dataset.
\end_layout

\begin_layout Standard
The main used metric for this Pixel-Level semantic classification results
 is the commonly known as the PASCAL VOC intersection-over-union metric
 which formula is presented in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Intersection over Union Metric"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
IoU=\frac{\#TP}{\#TP+\#FP+\#FN}\label{eq:Intersection over Union Metric}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\align center
with 
\begin_inset Formula $TP=TruePositives$
\end_inset

, 
\begin_inset Formula $FP=FalsePositives$
\end_inset

 and 
\begin_inset Formula $FN=FalseNegatives$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength
\backslash
extrarowheight{7pt}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Algorithm Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
iOU Category
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IoU Class
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\color blue
\begin_inset CommandInset href
LatexCommand href
name "motovis"
target "http://www.motovis.com/"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
91.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
81.0
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ResNet-38 
\begin_inset CommandInset citation
LatexCommand cite
key "wu2016wider"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
91.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NetWarp
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
91.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
PSPNet 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
90.6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
80.2
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cityscapes Dataset Challenge Results
\begin_inset CommandInset label
LatexCommand label
name "tab:Cityscapes Dataset Challenge Results"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Both ResNet-38 and PSPNet are based on convolutional networks.
 Since the presentation of AlexNet 
\begin_inset CommandInset citation
LatexCommand cite
key "krizhevsky2012imagenet"

\end_inset

 in 2012 those networks have been growing deeper and deeper each year ending
 up with 1,202 trainable layers.
\end_layout

\begin_layout Standard
The main difficulty of scene parsing is related to the type of scene and
 by all means to the label variety that one can predict.
 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

 has dealt with this problems assigning relationships between different
 dataset labels, i.e.
 a an airplane is likely to be in runway or flying in the sky while not
 over a road.
 This relationships will reduce slightly the complexity of having large
 amounts of labels to predict and will improve the general performance of
 the algorithm.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ResNet-38-Algorithm"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:PSPNet-Algorithm"

\end_inset

 we can observe some visual examples of how this two algorithms perform
 on some Cityscapes frames.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/ResNet-38 Results.png
	lyxscale 30
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ResNet-38 Algorithm
\begin_inset CommandInset label
LatexCommand label
name "fig:ResNet-38-Algorithm"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/State Of the Art/PSPNet Results.png
	lyxscale 30
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
PSPNet Algorithm
\begin_inset CommandInset label
LatexCommand label
name "fig:PSPNet-Algorithm"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Segmentation result examples on Cityscapes Dataset
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Multi-camera scenarios
\end_layout

\begin_layout Standard
The use of multi-camera is a common setup when dealing with video surveillance
 problems.
 One can define as multi-camera scenario one space that has more than one
 video camera working and recording at the same exact time.
 Having 
\begin_inset Formula $N$
\end_inset

 multiple camera instances allows to observe the same event or object of
 interest from up to two different points of view.
 This will lead to a set of advantages in the scope of our work when dealing
 with pedestrian detection and semantic classification and also, some unavoidabl
e disadvantages.
\end_layout

\begin_layout Itemize
Advantages
\end_layout

\begin_deeper
\begin_layout Standard
As said in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Pedestrian-detection-and crowd patrons"

\end_inset

 one of the main research paths in pedestrian detection is the occlusion
 treatment.
 In this case, the use of a multi-camera scenario with computed homographies
 between camera frames will help to reproject detections from one camera
 to another whose miss rates are high so one could solve partially the problem
 of occlusions or at least improve the performance.
 
\end_layout

\begin_layout Standard
When talking about semantic segmentation the inclusion of a multi-camera
 system will arise some benefits.
 While a single-camera system could lead to misclassification of labels
 in the image or even to represent an object with many classes due to for
 example, camera capture problems, in a multi-camera system one camera instance
 could help to refine the classes in another one provided that, evidentially,
 they are analyzing the same common area.
\end_layout

\end_deeper
\begin_layout Itemize
Disadvantages
\end_layout

\begin_deeper
\begin_layout Standard
The main disadvantage when dealing with multi-camera systems will be the
 exponential grow of computational time as algorithms should be performed
 
\begin_inset Formula $N$
\end_inset

 times being 
\begin_inset Formula $N$
\end_inset

 the number of camera instances.
 This issue could be solved by the use of some parallel coding that will
 perform cameras process separately.
\end_layout

\begin_layout Standard
Other disadvantages will be related with synchronization problems, as if
 the camera instances will be used to reproject objects/pedestrians from
 one camera to another they must evidentially represent the same exact moment
 in time in the same sequence frame, which depending on the setup will be
 more or less difficult.
\end_layout

\end_deeper
\end_body
\end_document
