#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle plain
\bullet 1 2 6 -1
\bullet 2 2 12 -1
\bullet 3 1 25 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Proposed System
\begin_inset CommandInset label
LatexCommand label
name "chap:Proposed-System"

\end_inset


\end_layout

\begin_layout Standard
During this Chapter our proposed system will be explained.
 We will start from the main contextual model generation and pedestrian
 detectors to finally explain the fusion of all the elements in a multi
 camera system in addition to the statistical usage data.
\end_layout

\begin_layout Section
Contextual Model Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Contextual-Model-Generation"

\end_inset


\end_layout

\begin_layout Standard
One of the main objectives in the scope of this work is to segment one frame
 into different semantic areas.
 The relative position in the scene for elements such as doors, walls, paths,
 columns and windows will be necessary to achieve further objectives such
 as multi camera fusion and statistical data.
 For this complex task we will use an algorithm presented in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

, PSP-Net 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

.
 The goal for this algorithm as said before, is is to assign each pixel
 in the image a category label and predict the label, location, as well
 as shape for each element.
 
\end_layout

\begin_layout Standard
This process is based on a deep convolutional neural network (CNN) called
 Pyramid Scene Parsing Network (PSPNet) which is designed to improve performance
 for open-vocabulary object and stuff identification in complex scene parsing.
 The structure of the network can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the proposed PSPNet
\begin_inset CommandInset label
LatexCommand label
name "fig:Overview-of-the proposed PSPNet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Given the image in (a) the first step is to process it through a pre trained
 CNN called ResNet 
\begin_inset CommandInset citation
LatexCommand cite
key "he2016deep"

\end_inset

 to get the feature map (b) of the last convolutional layer.
 The final feature map in this step is 
\begin_inset Formula $1/8$
\end_inset

 of the size of the input image.
 
\end_layout

\begin_layout Standard
The second step is to apply the main contribution of 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

, which is called the Pyramid Pooling Module (c).
 The main objective of the module is to collect a few levels of information,
 much more representative than global pooling.
 It separates the feature map into different sub-regions and forms pooled
 representations for different locations.
 Here a set of pooling, convolutional and upsampling layers are applied
 that will harvest different sub-region representation in 
\begin_inset Formula $N$
\end_inset

 different scales.
 
\end_layout

\begin_layout Standard
After the bilinear interpolation upsampling, concatenation layers are used
 to form the final feature representation by fusing the feature map extracted
 in (b) and the Pyramid Pooling Module output.
 This final feature carries both local and global context information.
 In the end, the representation is fed into a convolutional layer which
 gets the final per-pixel prediction (d).
\end_layout

\begin_layout Standard

\color blue
\begin_inset CommandInset href
LatexCommand href
name "PSP-Net"
target "https://github.com/hszhao/PSPNet"

\end_inset

 
\color inherit
comes with a set of three different pre trained Caffe models for three different
 datasets.
 The main difference between the models for our scope is the environment
 in which the network has been trained.
\end_layout

\begin_layout Itemize
ADE20K: This dataset is the most challenging as it has up to 150 different
 labels in a wide range of scenes.
 The scenes go from interior room places to outdoor scenarios.
\end_layout

\begin_layout Itemize
VOC2012: It contains 20 object categories and one background class from
 diverse indoor and outdoor scenes.
\end_layout

\begin_layout Itemize
CityScapes: The last dataset defines 19 categories containing both stuff
 and objects.
 All the available sequences have been recorded from a driving car while
 driving in the street.
\end_layout

\begin_layout Standard
So, as one can observe the three different models represent different object
 categories in different real spaces.
 In our case we will select the model based on two main reasons:
\end_layout

\begin_layout Enumerate
The model should be trained with indoor scenes.
 This will lead to discard those models that perform only in outdoors scenes
 as we would like our approach to be used in interior scenarios.
 This will exclude CityScapes dataset from our options as all the classes
 and sequences used for training are from outdoor scenes.
\end_layout

\begin_layout Enumerate
From the trained indoor models we have to choose between those whose categories
 fit best in our work.
 In this case VOC2012 dataset uses classes such as boat, airplane or table
 which are not interesting for our segmentation and it does not have classes
 such as door or wall which are really important for us.
\end_layout

\begin_layout Standard
With this two reasons being taken into account we have selected the model
 ADE20K because it has elements such as walls, floor, person and column
 in its model.
 However, we consider that most of the 150 label categories will be unused
 in our procedure, so, the number of classes in the model has been reduced
 to the 21 classes of our interest so we only obtain scores for those objects
 that we want.
 This class limitations will lead above all in a considerably hard drive
 space saving.
 In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset

 can be observed the final 21 classes that have been used.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
wall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
building
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
floor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ceiling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
road
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
window pane
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
person
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
chair
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
seat
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
desk
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
lamp
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
column
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
counter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
path
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
screen door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairway
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
toilet
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
poster
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
bag
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Final classes list from ADE20K to use in PSP-Net
\begin_inset CommandInset label
LatexCommand label
name "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Pedestrian Detection
\end_layout

\begin_layout Standard
Along this section we will present the pedestrian State of the Art detectors
 that have been selected for our system.
 Some of them have been chosen due to the algorithm simplicity and computational
 cost and others due to its contrasted performance while working in real
 sequences as we have seen in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

.
 However, one of the main reasons of choosing the following three algorithms
 among others in the SoA is the possibility of either having the original
 source code or the practical implementation within an external library.
\end_layout

\begin_layout Subsection
Histogram of Oriented Gradients
\end_layout

\begin_layout Standard
Histogram of Oriented Gradients, i.e.
 HOG, is one of the main used detectors along pedestrian detection field.
 This fact is due to it's extremely simplicity in terms of the descriptor
 complexity.
 Pedestrians are described as set of HOG, this means that its shape and
 appearance can be described by a set of gradients and intensities organized
 as orientation histograms.
 These histograms will describe intensity distributions from local gradients
 or border directions.
 This descriptor can be observe graphically in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:HOG-Pedestrian-Descriptor"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/HoG.jpg
	lyxscale 60
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
HOG Pedestrian Descriptor
\begin_inset CommandInset label
LatexCommand label
name "fig:HOG-Pedestrian-Descriptor"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once HOG have been used to describe the person shape, Support Vector Machines
 are used to train a person model and to classify potential candidates as
 people.
 SVM are a data classification method formed by a set of supervised training.
 The aim of this kind of approaches is to produce a model which will be
 able to predict classification labels on a test set based only on the descripto
rs of the set.
\end_layout

\begin_layout Standard
The idea behind SVM is that training vectors will be mapped on to a bigger
 dimensional space in which the data separation by means of one or many
 hyperplane will be much easier than in the original dimensional space.
\end_layout

\begin_layout Standard
The combination between HOG and SVM will lead to a fast detector that depending
 on the situation will perform decently, although it has some main drawbacks
 as its lack of occlusion treatment which will not make this algorithm usable
 when working with crowded spaces.
\end_layout

\begin_layout Standard
The main implementation of Histogram of Oriented Gradient Pedestrian Detector
 is in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Deformable Part Model
\end_layout

\begin_layout Standard
As was mentioned in the previous paragraph one of the main drawbacks when
 working with the simple HOG pedestrian detector is that it describes the
 person model as a hole which will lead, inevitably, to the described drawbacks.
 Deformable Part Model tries to solve this problem, among others, by defining
 the model as first, a global coarse template, secondly, several higher
 resolution part templates and finally a spatial model for the location
 of each part.
 This description is the one that can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-detection-obtained with DPM Person model"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/DPM.png
	lyxscale 20
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Example detection obtained with the DPM person model.
\end_layout

\end_inset

Example detection obtained with the DPM person model.
 From left to right: Original image + detections, global coarse model, part
 templates and spatial model for each part.
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-detection-obtained with DPM Person model"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "felzenszwalb2010object"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Both global and part templates are modeled with histogram of gradient features
 and the model is built by using an improving over SVM called latent SVM.
 In addition, scores for every detection are obtained by applying a root
 filter on the window plus the sum over parts of the maximum over placements
 of that par, of the part filter score on the resulting sub window minus
 the deformation cost.
\end_layout

\begin_layout Standard
The use of this detector will lead to a set of advantages than when working
 with others simpler approaches.
 In terms of pedestrian occlusion treatment we will be able to detect those
 people that have been occluded by something in the scene just by detecting
 some visible part.
 This will outperform other detectors while working with crowded scenes
 in which holistic methods will have problems that will lead for instance
 to groups of people being detected as a unique detection while DPM will
 be able to separate them into different person instances.
\end_layout

\begin_layout Standard
However, its scanning window approach as well as the part based model will
 lead to some computational cost that will increase the time needed to obtain
 detections.
\end_layout

\begin_layout Standard
The main implementation of Deformable Part Model Pedestrian Detector can
 be found in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Aggregated Channel FeaturesDetector 
\end_layout

\begin_layout Standard
The basic idea behind ACF detector is to outperform other approaches by
 the use of many different channels on an input image I.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ACF Architecture"

\end_inset

 one can observe the working path of the mentioned detector.
 Given an initial image 
\begin_inset Formula $I$
\end_inset

, several channels are computed.
 This channels will be named 
\begin_inset Formula $C=\Omega(I)$
\end_inset

.
 After the computation, every block of pixels in 
\begin_inset Formula $C$
\end_inset

 is summed and the resulting lower resolution channels are smoothed.
 After a vectorizing process features will be single pixel lookups in the
 aggregated channels.
 Boosting trees are then used to learn this features (pixels) in order to
 distinguish people from the background.
 It is quite evidentially that the use of the correct channels will lead
 to better results.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset

 the set of 10 channels used to achieve state-of-the-art performance in
 pedestrian detection are:
\end_layout

\begin_layout Enumerate
Normalized gradient magnitude (1 channel).
\end_layout

\begin_layout Enumerate
Histogram of oriented gradients (6 channels).
\end_layout

\begin_layout Enumerate
LUV color channels (3 channels).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/ACF Detector.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Aggregated Channel Features architecture.
\end_layout

\end_inset

Aggregated Channel Features architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:ACF Architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Fast Region-Based Convolutional Network
\end_layout

\begin_layout Standard
Fast Region-based Convolutional Network (Fast R-CNN) will be used to detect
 pedestrian in our proposed system, however, as said in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 it is not only a pedestrian detector but an algorithm for object detection.
 In 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Object-Proposals"

\end_inset

 we mentioned that Fast-RCNN must use objects proposals for its usage.
 In our developed system MCG 
\begin_inset CommandInset citation
LatexCommand cite
key "arbelaez2014multiscale"

\end_inset

 grouping method will be used for this purpose due to its great results.
\end_layout

\begin_layout Standard
Fast-RCNN method sets its contributions in a several number of innovations
 to improve training and testing speed over its fundamental base R-CNN:
\end_layout

\begin_layout Enumerate
Higher detection quality (mAP).
\end_layout

\begin_layout Enumerate
Training is single-stage, using a multi-task loss.
\end_layout

\begin_layout Enumerate
Training can update all network layers.
\end_layout

\begin_layout Enumerate
No disk storage is required for feature caching.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast-R-RCNN-architecture"

\end_inset

 one can observe the general Fast R-RCNN architecture.
 The input for the Fast-RCNN network are the entire desired image that one
 wants to process and the set of object proposals.
 The first step in the network is to process the whole image with several
 convolutional and max pooling layers to produce a convolutional feature
 map.
 Then, for every object proposal present in the image, a region of interest
 (ROI) pooling layer extracts a fixed-length feature vector from the feature
 map.
\end_layout

\begin_layout Standard
Each feature vector is after, introduced into a sequence of fully connected
 (FC) layers that finally diverge into to output layers.
 The first one produces probabilities estimates over 
\begin_inset Formula $K$
\end_inset

 object classes.
 The second one outputs four real numbers for each of the 
\begin_inset Formula $K$
\end_inset

 object classes.
 This set of 4 values encodes the final bounding-box positions for one of
 the objects from the 
\begin_inset Formula $K$
\end_inset

 classes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Fast RCNN.png
	lyxscale 10
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Fast R-RCNN architecture.
\end_layout

\end_inset

Fast R-RCNN architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "girshick2015fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Fast-R-RCNN-architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this case, both Fast-RCNN detector and MCG object proposal extract are
 implemented within external Matlab libraries.
\end_layout

\begin_layout Subsection
PSP-Net
\end_layout

\begin_layout Standard
As we explained in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Contextual-Model-Generation"

\end_inset

 PSP-Net working along with ADE20K dataset has been trained to detect people
 as a class.
 In our proposed system we will also use this approach along with the previously
 explained pedestrian algorithms.
\end_layout

\begin_layout Section
Fusion and Filtering
\end_layout

\begin_layout Standard
Once the semantic model and pedestrian detections are implemented the next
 step is to combine the information coming from different camera sources
 in a process that we have called fusion and filtering.
\end_layout

\begin_layout Subsection
Multi-camera Scenario
\end_layout

\begin_layout Standard
Our system is developed in a multi-camera scenario.
 This means that we will be able to observe the scene from different point
 of views.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration"

\end_inset

 we can observe how the scene is configured with 3 different video-surveillance
 cameras.
 Two of them are placed at the sides of the scene, while one is at the bottom
 part.
 The starting views from the three cameras is displayed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Configuration Static.png
	lyxscale 30
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 1 Initial View.jpg
	lyxscale 50
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 2 Initial View.jpg
	lyxscale 50
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 3 Initial View.jpg
	lyxscale 50
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial Camera Views
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial Camera Views"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As said in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 working with a multi-camera scenario has some advantages.
 As one can see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 we can observe the same scene area from three different point of views,
 which for example means, that detections from one camera can be used to
 detect in other camera or even refine the available detections.
 
\end_layout

\begin_layout Subsubsection
Cenital Plane Homography
\end_layout

\begin_layout Standard
Homography calculation is a pivotal task in our work and the main base for
 the fusion of all the different information coming from the three cameras.
 The objective is to compute an homography matrix that will relate a camera
 frame to a so called cenital plane, i.e.
 a bird-eye representation of the scene.
 With this homography matrix we will be able to transform every frame pixel
 to its correspondent position in the cenital plane, i.e.
 it will transform the frame as if it were being viewed from the top.
\end_layout

\begin_layout Standard
In order to compute an homography, a relation between at least 4 points
 in each of the two images should be computed.
 In this case we are trying to create a relationship between a real camera
 frame, and a cenital view plane created by computer.
 This means that there will not be a real correspondence for pixels and
 so it is not possible to use a point descriptor to extract common points
 in both images.
 User has to manually select the points that represent the same spatial
 place in both images using the Graphical User Interface and then the algorithm
 will compute the transformation matrix.
\end_layout

\begin_layout Subsubsection
Cenital Plan Design
\end_layout

\begin_layout Standard
As said in the previous paragraph, one of the main objectives of the work
 is to project the extracted detections from the three cameras, i.e.
 pedestrian and semantic, into a common plane for all of them.
 To achieve this goal a cenital plane from the scene is needed.
 The first approach used for the cenital plane can be observe in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:First cenital plane approach"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/First Approach CenitalView.png
	lyxscale 30
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
First cenital plane approach
\begin_inset CommandInset label
LatexCommand label
name "fig:First cenital plane approach"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can observe the first approach of the cenital plane lacks of details
 from the scene.
 The information about the details of the scenario is minimum and also,
 the scene proportions are not correct.
 To compute a correct homography between the camera frame and the cenital
 plane we should be able to identify the same scene points in both images
 in the ground plane.
 This means that the cenital plane should have enough detail so the point
 selection is done correctly by the user and the homography is correctly
 computed.
\end_layout

\begin_layout Standard
For this reason another cenital plane has been compute starting from zero.
 In this new approach the scene has been correctly measure by hand and the
 plane has been done with real measures and high floor detail.
\end_layout

\begin_layout Standard
For correctly drawing the plane 
\color blue

\begin_inset CommandInset href
LatexCommand href
name "AutoCAD 2017"
target "https://www.autodesk.es/products/autocad/overview"

\end_inset


\color inherit
 software has been used.
 The second plane approach can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Second cenital plane approach with measures"

\end_inset

 with all the manual measures extracted from the real scene and in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Second cenital plane approach with camera positions"

\end_inset

 with the correct camera positions.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Measured Cenital Plane.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Second cenital plane approach with real measures
\begin_inset CommandInset label
LatexCommand label
name "fig:Second cenital plane approach with measures"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Second Approach Cenital View.png
	lyxscale 30
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Second cenital plane approach with camera positions
\begin_inset CommandInset label
LatexCommand label
name "fig:Second cenital plane approach with camera positions"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is easily observable that differences between Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:First cenital plane approach"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Second cenital plane approach with camera positions"

\end_inset

 are outstanding both in floor details and in general construction proportions.
 This detail increase will lead to a much more easy homography selection
 points by the user as it now has much more point options to choose and
 evidentially this will mean that the final homography matrix, and so, all
 the projections will be increased accurately talking.
\end_layout

\begin_layout Subsubsection
Camera Panning
\end_layout

\begin_layout Standard
When cameras are static and pointing to the center of the scene as in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 one can easily observe that the vision range compared to the hole scenario
 representation is quite limited due to the low vision range of the cameras.
 The cameras are covering the middle part leaving completely unattended
 the lateral parts of the scenario.
 The solution to this problem is to include panning movement in all the
 cameras from left to right so cameras view range is increased and no longer
 focus in the middle of the scene as before.
 This will lead also that the common areas that the cameras will share are
 increased and not static.
 This solution is presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration with panning set up"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Panning Configuration.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration with panning set up
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration with panning set up"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Camera Panning Discretization
\end_layout

\begin_layout Standard
Before including camera panning in the set up, homographies where calculated,
 at the beginning, for only one video frame per camera.
 This means that every camera only had one homography matrix as all its
 sequence was static and all its video frames where projected using the
 same matrix.
 However, when including video panning this solution is no longer valid
 as the scene view is constantly changing.
 The ideal solution for this problem would be having one homography matrix
 
\begin_inset Formula $H$
\end_inset

 per video frame.
 As the homographies are calculated by the user selecting the points this
 solution is evidentially impossible in terms of usability.
 We propose discretization of the panning tour in which 
\begin_inset Formula $N$
\end_inset

 views are selected from the video sequence.
 This will lead to an homography codebook in where the user will compute
 a homography matrix 
\begin_inset Formula $H$
\end_inset

 for each of the 
\begin_inset Formula $N$
\end_inset

 camera views.
 This codebook of views and homographies will be responsible for projecting
 into the cenital plane the hole video.
\end_layout

\begin_layout Subsubsection
View Selection
\end_layout

\begin_layout Standard
Due to the creation of the homography codebook we now have to choose between
 a set of 
\begin_inset Formula $N$
\end_inset

 homography matrices to project detections into the cenital plane.
 The actual analyzed frame should be compared to each of the 
\begin_inset Formula $N$
\end_inset

 views to obtain an spatial correspondence and so, use the correct homography
 matrix.
\end_layout

\begin_layout Standard
In order to compare two images we have done comparison between points of
 interest.
 AKAZE detector and descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "alcantarilla2011fast"

\end_inset

 has been used for this task.
\end_layout

\begin_layout Standard
AKAZE will detect and describe points of interest in any image and then
 by a brute force comparator we will extract how many coincidence we have
 between a pair of images.
 It is essential to say, that, as we have to compare the current frame with
 all of the 
\begin_inset Formula $N$
\end_inset

 vies we will have a trade off.
 More views will mean a better space discretization, however, the computational
 time will increase exponentially, this means that we have to choose the
 number of views 
\begin_inset Formula $N$
\end_inset

 so it represents correctly the space and keeps the computational time relativel
y low.
\end_layout

\begin_layout Standard
As we said before, the homographies are calculated for each of the views,
 this means, that if a frame is not positioned exactly as a view the homography
 matrix will not project the points correctly, there will be a small error.
 Taking advantage of the calculation of common points of interest between
 the frame and its correspondent view we can solve this problem by calculating
 automatically the homography between both images.
 This means that now, to project detections from the current frame to the
 cenital plane, first we will change the perspective of it to the perspective
 of the view.
 This way we will ensure that the used perspective is the same one as the
 one that was used to compute the homography.
 This process is graphically explained in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:View selection process and homography between views computation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/View Selection Process.png
	lyxscale 10
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
View selection process and homography calculation.
\end_layout

\end_inset

View selection process and homography between views computation.
 
\begin_inset Formula $N_{Views}=5$
\end_inset

.
 
\color red
IMAGEN NO TERMINADA Y PENDIENTE DE CAMBIOS
\color inherit

\begin_inset CommandInset label
LatexCommand label
name "fig:View selection process and homography between views computation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Video Sequence Synchronization
\end_layout

\begin_layout Standard
One of the main issue when dealing with multi camera systems, and specially,
 those that will combine information between the cameras, is that video
 sequences coming from them should be correctly synchronized.
 This is really important because if the same frame number 
\begin_inset Formula $f$
\end_inset

 is not representing the same exact moment in time in the three cameras,
 combination and fusion of the information is not possible.
\end_layout

\begin_layout Standard
We have synchronized the processed videos using the so called clapperboard
 technique.
 This technique aims to synchronize using concrete events that can be observed
 from all the cameras at the same time.
 By this, we will be able to set a synchronize starting point from where
 the following video frames will be correctly representing the same moment
 in time.
\end_layout

\begin_layout Subsection
Pedestrian Filtering and Constraining
\end_layout

\begin_layout Standard
During this Section we will explain how the pedestrian detections are filtered
 and also how the information between people from the three cameras is fusion
 in the system to increase the general algorithm performance.
\end_layout

\begin_layout Subsubsection
Cylinder Estimation
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "Miguelez2016Deteccion"

\end_inset

 a cylinder estimation technique is proposed.
 The detected bounding boxes on the camera frame do not correspond spatially
 with the exact position of the detected object due to the camera perspective.
 If this detection error is not corrected when the bounding boxes are projected
 either to another camera instance or to the common cenital plane there
 will be a distance between the represented bounding box and the real object.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for camera instances projections"

\end_inset

 shows the case for bounding box transference between cameras and Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for cenital view projections"

\end_inset

 for the cenital frame.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cylinder estimation between cameras.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cylinder estimation for camera instance projections
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for camera instances projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cylinder estimation for cenital view projections
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for cenital view projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can observe in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for camera instances projections"

\end_inset

 when the blue bounding boxes are projected from image (a) to image (c)
 they are not correctly on the pedestrian.
 The solution is to compute the cylinder that embrace the square whose side
 is the bounding box (b).
 Once the cylinder is estimated, the person will be hypothetically in the
 middle of the cylinder.
 In (c) we can observe that the estimation, if correctly computed has great
 accuracy.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for cenital view projections"

\end_inset

 the same method is applied but in this case, the bounding box is not projected
 to another camera perspective but to the cenital plane.
 In this case we can see that the projection of the bounding box, represented
 by the green line is not in the same position as the center of the cylinder,
 which will be at the end of the purple line.
 The center of the cylinder so, will correspond to a more approximate position
 of the detected person.
\end_layout

\begin_layout Standard

\color red
Poner aqui o en resultados?
\end_layout

\begin_layout Subsubsection
Gaussian Bounding Box Representation
\end_layout

\begin_layout Standard
Using also the cylinder representation one can change the way bounding boxes
 are represented in the cenital plane.
 As said before, in the simple cylinder representation pedestrian will be
 represented with two perpendicular lines, however, in this representation
 one does have spatial information about the detection but none about the
 detection accuracy.
 This means that with the two perpendicular lines one can observed where
 the detection has been achieved but not the score associated with this
 detections.
 To solve this issue we propose a new representation method based on a Gaussian
 function placed at the middle of the cylinder.
\end_layout

\begin_layout Standard
Every pedestrian detection will be so represented as a Gaussian function
 of the form described in Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Gaussian Function Equation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x,y)=Aexp\left(-\left(\frac{(x-x_{0}){}^{2}}{2\sigma_{x}^{2}}+\frac{(y-y_{0}){}^{2}}{2\sigma_{y}^{2}}\right)\right)\label{eq:Gaussian Function Equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case 
\begin_inset Formula $A$
\end_inset

 will be the amplitude which typically will be 
\begin_inset Formula $A=1$
\end_inset

, 
\begin_inset Formula $x_{0},\:y_{0}$
\end_inset

 will represent the mean of the gaussian which, in our case, will be the
 center of the cylinder associated to the bounding box and 
\begin_inset Formula $\sigma_{x},\sigma_{y}$
\end_inset

 which is the standard deviation and in our case will represent the accuracy
 of the detection.
 
\end_layout

\begin_layout Standard

\color red
EXPLAIN THE RULE THAT RELATES SCORE AND STANDARD DEVIATION
\end_layout

\begin_layout Standard
This means that detections with higher scores will be represented as a narrow
 gaussian function (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:High-score-detections"

\end_inset

) while lower scores will lead to wide gaussians that will represent a bigger
 area in where one can find that person (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Low-score-detections"

\end_inset

).
 Some Gaussians examples depending on the scores can be observed in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gaussian-representation-examples"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Gaussian Representation 2.png
	lyxscale 20
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
High score detection.
 Frame 86
\begin_inset CommandInset label
LatexCommand label
name "fig:High-score-detections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Gaussian Representation 1.png
	lyxscale 20
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Low score detections.
 Frame 87
\begin_inset CommandInset label
LatexCommand label
name "fig:Low-score-detections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Gaussian representation examples.
\begin_inset CommandInset label
LatexCommand label
name "fig:Gaussian-representation-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color red
Aqui o en resultados?
\end_layout

\begin_layout Subsubsection
Pedestrian Reprojection Between Cameras
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-Reprojection between cameras"

\end_inset


\end_layout

\begin_layout Standard
When all the pedestrian detections have been projected into the cenital
 plane we can start reprojecting those projections from the cenital plane
 back to the camera frames.
 This process will have a fundamental important as it will be the process
 from where the pedestrian detector accuracy can be increase by the use
 of the multi-camera system.
 For instance, detections from Cameras 2 and 3 will be reprojected into
 Camera 1.
 Sometimes, those reprojections will not be in the frame because the cameras
 will not be aiming to the same spatial area, however, when the are seeing
 the same spatial space cameras will share those detections.
 Ideally, if the three cameras detect the person reprojection will not be
 necessary, but, if one of them misses the detection, the other two detections,
 by the use of reprojection, will lead to suppress that miss detection.
 This process can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrain reprojection into other cameras"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1 + Reprojections from 2 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hspace{}
\length 10text%
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2 + Reprojections from 1 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3 + Reprojections from 1 and 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Pedestrian detection reprojection
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrain reprojection into other cameras"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Pedestrian Semantic Constraining
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-Semantic-Constraining"

\end_inset


\end_layout

\begin_layout Standard

\color red
AQUI SE DA POR EXPLICADO EL PROCEDIMEINTO PARA CALCULAR ZONAS COMUNES DE
 SUELO ENTRE PARES DE CAMARAS.
\end_layout

\begin_layout Standard
Once all the detections have been reprojected into all the cameras we will
 use semantic information in order to filter those bounding boxes that hypotheti
cally do not represent pedestrians.
 Ideally people are always walking on the floor so we will use the common
 floor areas between pairs of cameras to constrain the pedestrian detections
 to this hypothesis.
 This process will be done based on the cenital plane and using the cylinder
 estimation explained before.
 The constrain idea will be that a bounding box coming from a camera 
\begin_inset Formula $C_{1}$
\end_inset

 is assumed correctly computed if the center of its related cylinder is
 on the common floor between 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 and also between 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{3}$
\end_inset

.
 An example of the constraining can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrain semantic constraining"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hspace{}
\length 10text%
\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Pedestrian semantic constraining.
\end_layout

\end_inset

Pedestrian semantic constraining.
 Blobs that are not in the right semantic area are displayed with a text
 label that says 
\begin_inset Quotes eld
\end_inset

Blob Suppressed
\begin_inset Quotes erd
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrain semantic constraining"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Reference and Inertial Homography Planes
\end_layout

\begin_layout Standard
During this Section we will explain our proposed method to create both RGB
 and semantic reference plane combining information from all the cameras
 positions.
 The final objective is to have complete maps in each camera that represent
 the scene.
\end_layout

\begin_layout Subsection
Semantic and RGB Reference Plane Generation
\end_layout

\begin_layout Standard
Following with the process scheme of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:View selection process and homography between views computation"

\end_inset

 one can project every video frame into a cenital perspective.
 This way a set of projected RGB and semantic frames for each camera will
 be obtained.
 Some examples of these images can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGB-and-semantic projected"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame207 RGB.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Projected 
\begin_inset Formula $Frame_{t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame1169 RGB.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Projected 
\begin_inset Formula $Frame_{t+\Delta t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame207 Semantic.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Projected 
\begin_inset Formula $Frame_{t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame1169 Semantic.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Projected 
\begin_inset Formula $Frame_{t+\Delta t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB and semantic frames projected.
\begin_inset CommandInset label
LatexCommand label
name "fig:RGB-and-semantic projected"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once all the video frames are projected we propose to apply a temporal average
 through a median filter for the three cameras separately.
 By this method a reference plane contained in the ground plane will be
 created by joining all the separated projected frames.
 This process is detailed in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Median Temporal Average"

\end_inset

(COMPLETAR ECUACION).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
MedianSemantic_{C}=\sum_{i=1}^{nFrames}Projected_{i}\label{eq:Median Temporal Average}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The result coming from the temporal averaging can be observed in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGB Median Average"

\end_inset

 for RBG case and in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Semantic Median Average"

\end_inset

 for semantic information.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB1Median.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB2Median.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB3Median.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Median Average.
\begin_inset CommandInset label
LatexCommand label
name "fig:RGB Median Average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem1Median.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem2Median.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem3Median.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Median Average.
\begin_inset CommandInset label
LatexCommand label
name "fig:Semantic Median Average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As one can observe we get smooth semantic maps from all the hall area as
 well as an RGB map.
 It is important to recall that only the ground floor is correctly projected
 and also, the base of for instance the columns from all the different cameras
 should lay in the same spatial point in these images.
\end_layout

\begin_layout Subsection
Semantic Fusion in the Reference Plane
\end_layout

\begin_layout Standard
Once the cenital maps have been extracted one can combine them to create
 semantic fusion in the reference plane.
 The main objective is to obtain common areas, i.e.
 pixels with the same label within two or more cameras.
 This method will allow us to increment the confidence for those pixels
 and so, we can use this information for example, to constrain pedestrians
 detections.
 As the reference plane is obtained by the homography to the ground plane
 we have only take into account floor pixels to create the common areas,
 as the others will have projection errors that will lead to fusion errors.
\end_layout

\begin_layout Standard
First, we have combine pairs of cameras to create three common semantic
 areas that are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas between pair of cameras."

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic12.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cameras 1 and 2
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic23.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cameras 2 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic13.png
	lyxscale 20
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cameras 3 and 1
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Common semantic areas between pair of cameras.
\begin_inset CommandInset label
LatexCommand label
name "fig:Common semantic areas between pair of cameras."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once pairs of cameras have been combined one can go further and extract
 common areas for the three cameras at the same time.
 Those areas will be pixels with the exact same label for the three cameras
 so its probability to be that class is real high.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas for all the cameras"

\end_inset

 represents the common areas.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemanticAllCameras.png
	lyxscale 20
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Common semantic areas for all the cameras.
\begin_inset CommandInset label
LatexCommand label
name "fig:Common semantic areas for all the cameras"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can observe, due to the multi camera setup and some occlusions the
 common area for all the cameras represent a small portion of the room.
 This is the reason why we have choose common areas between pairs of cameras
 to work along with pedestrian detection rather than common area between
 all of them.
\end_layout

\begin_layout Subsection
Parametric Homographies Between Inertial Planes
\end_layout

\begin_layout Standard
Generacion de planos paralelos al cenital para proyectar nuevos puntos comunes
 a otra altura.
\end_layout

\begin_layout Section
Statistical Usage Data
\end_layout

\begin_layout Standard
Along this Section the extraction of the statistical usage data will be
 deeply explained.
\end_layout

\end_body
\end_document
