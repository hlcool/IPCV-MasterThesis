#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle plain
\bullet 1 2 6 -1
\bullet 2 2 12 -1
\bullet 3 1 25 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Proposed System
\begin_inset CommandInset label
LatexCommand label
name "chap:Proposed-System"

\end_inset


\end_layout

\begin_layout Standard
During this Chapter our proposed system is analyzed.
 We start from the contextual model generation and pedestrian detectors.
 Then, the fusion of all the obtained elements in a multi camera system
 and semantic filtering are described.
 Finally, a case of example which generates statistical usage data from
 semantic areas is proposed.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Proposed-system-flowchart"

\end_inset

 depicts the flowchart of the modular proposed method.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/System FlowChart.svg
	lyxscale 30
	width 100text%
	height 100theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Flowchart of the proposed method
\begin_inset CommandInset label
LatexCommand label
name "fig:Proposed-system-flowchart"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Contextual Model Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Contextual-Model-Generation"

\end_inset


\end_layout

\begin_layout Standard
One of the main objectives in the scope of this work is to perform semantic
 segmentation, which 
\begin_inset Formula $-$
\end_inset

as explained in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset


\begin_inset Formula $-$
\end_inset

targets to divide one frame onto different semantic areas.
 The relative position in the scene for elements such as doors, walls, paths,
 and columns is required to achieve further objectives such as multi camera
 pedestrian constraint and statistical data extraction.
 
\end_layout

\begin_layout Standard
For this complex task the algorithm PSP-Net 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

 presented in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 is used.
 We choose PSP-Net because at the moment of this work was the one with available
 code achieving the best results (see Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Cityscapes Dataset Challenge Results"

\end_inset

).
 The goal of this algorithm is is to assign each pixel in the image a category
 label.
\end_layout

\begin_layout Subsection
Pyramid Scene Parsing Network
\end_layout

\begin_layout Standard
It uses a deep Convolutional Neural Network (CNN) called Pyramid Scene Parsing
 Network (PSPNet).
 This network is designed to improve performance for open-vocabulary object
 identification in complex scene parsing.
 The structure of the network is represented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the proposed PSPNet
\begin_inset CommandInset label
LatexCommand label
name "fig:Overview-of-the proposed PSPNet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Algorithm Stages
\end_layout

\begin_layout Standard
The algorithm is divided onto different stages:
\end_layout

\begin_layout Enumerate
Image in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(a) is processed it through a pre trained CNN called ResNet 
\begin_inset CommandInset citation
LatexCommand cite
key "he2016deep"

\end_inset

.
 The objective is to get the full feature map 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(b) of the last convolutional layer.
 The final feature map in this step is 
\begin_inset Formula $1/8$
\end_inset

 of the size of the input image.
 
\end_layout

\begin_layout Enumerate
Apply the main contribution of 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

, called the Pyramid Pooling Module 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(c).
 The main objective of the module is to collect a few levels of information,
 much more representative than global pooling.
 It separates the feature map onto different sub-regions and forms pooled
 representations for different locations.
 Here a set of pooling, convolutional and upsampling layers are applied
 to harvest different sub-region representation in 
\begin_inset Formula $N$
\end_inset

 different scales.
 
\end_layout

\begin_layout Enumerate
Concatenation layers are used to form the final feature representation by
 fusing the feature map extracted in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(b) and the Pyramid Pooling Module output.
 This final feature carries both local and global context information.
 
\end_layout

\begin_layout Enumerate
The representation is fed onto a convolutional layer which gets the final
 per-pixel prediction 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(d).
\end_layout

\begin_layout Subsubsection*
Semantic Segmentation Particularization
\end_layout

\begin_layout Standard

\color blue
\begin_inset CommandInset href
LatexCommand href
name "PSP-Net"
target "https://github.com/hszhao/PSPNet"

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout

\color blue
https://github.com/hszhao/PSPNet
\end_layout

\end_inset

 
\color inherit
comes with a set of three different pre trained 
\color blue

\begin_inset CommandInset href
LatexCommand href
name "Caffe"
target "http://caffe.berkeleyvision.org/"

\end_inset


\shape italic

\begin_inset Foot
status collapsed

\begin_layout Plain Layout

\color blue
http://caffe.berkeleyvision.org/
\end_layout

\end_inset


\shape default
\color inherit
 models for three different datasets.
 The main difference between the models for our scope is the environment
 in which the network has been trained.
\end_layout

\begin_layout Itemize
ADE20K: This dataset is the most challenging as it has up to 150 different
 labels in a wide range of scenes.
 The scenes go from interior room places to outdoor scenarios.
\end_layout

\begin_layout Itemize
VOC2012: It contains 20 object categories and one background class from
 diverse indoor and outdoor scenes.
\end_layout

\begin_layout Itemize
CityScapes: The last dataset defines 19 categories containing both stuff
 and objects.
 All the available sequences have been recorded from a driving car while
 driving in the street.
\end_layout

\begin_layout Subsubsection*
Model Particularization
\end_layout

\begin_layout Standard
As one can observe the three different models represent different object
 categories in different real spaces.
 In our case we select the model based on two main reasons:
\end_layout

\begin_layout Enumerate
The model should have been trained with indoor scenes.
 This leads to discard those models that represent only outdoors scenes
 as we would like our approach to be used in an interior scenario.
 This will exclude CityScapes dataset from our options as all the classes
 and sequences used for training are from outdoor scenes.
\end_layout

\begin_layout Enumerate
From the trained indoor models we have to choose between those whose categories
 best fit in our work.
 In this case VOC2012 dataset uses classes such as boat, airplane or table
 which are not interesting for our segmentation problem and it does not
 have classes such as door or wall which are really important for us.
\end_layout

\begin_layout Subsubsection*
Selected Model
\end_layout

\begin_layout Standard
Considering these two reasons, we have selected the model ADE20K because
 as said, it has elements such as walls, floor, person and column in its
 model.
 
\end_layout

\begin_layout Standard
However, we consider that most of the 150 label categories will be unused
 in our procedure, so, the number of classes from the model has been reduced
 to the 21 classes of our interest.
 Position and scores for those objects are the only ones obtained.
 This class limitation leads above all in a considerably hard drive space
 saving.
 In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset

 the final 21 selected classes are exposed.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength
\backslash
extrarowheight{7pt}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
wall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
building
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
floor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ceiling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
road
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
window pane
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
person
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
chair
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
seat
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
desk
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
lamp
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
column
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
counter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
path
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
screen door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairway
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
toilet
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
poster
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
bag
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Final classes list from ADE20K to use in PSP-Net
\begin_inset CommandInset label
LatexCommand label
name "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Pedestrian Detection
\end_layout

\begin_layout Standard
Along this section pedestrian State of the Art detectors that have been
 integrated in the proposed system are presented.
 Some of them have been chosen due to their efficiency, whereas some have
 been chosen due to its contrasted good performance (see Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

).
\end_layout

\begin_layout Standard
Besides, algorithm source code of all the chosen approaches is available.
\end_layout

\begin_layout Standard
However, one of the main reasons of choosing the following five algorithms
 among others in the SoA is the possibility of either having the original
 source code or the practical implementation within an external OpenCV library.
\end_layout

\begin_layout Subsection
Histogram of Oriented Gradients
\end_layout

\begin_layout Standard
Histogram of Oriented Gradients, i.e.
 HOG, is one of the main used detectors along pedestrian detection field.
 This fact is due to it's extremely simplicity in terms of the descriptor
 complexity.
 
\end_layout

\begin_layout Subsubsection*
Person Descriptor
\end_layout

\begin_layout Standard
Pedestrians are described as set of HOG.
 This means that its shape and appearance can be described by a set of gradients
 and intensities organized as orientation histograms.
 These histograms describe intensity distributions from local gradients
 or border directions.
 This descriptor can be observed graphically in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:HOG-Pedestrian-Descriptor"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/HoG.jpg
	lyxscale 60
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
HOG Pedestrian Descriptor
\begin_inset CommandInset label
LatexCommand label
name "fig:HOG-Pedestrian-Descriptor"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Model Generation
\end_layout

\begin_layout Standard
Once HOG have been used to describe the person shape, Support Vector Machines
 are used to train a person model and to classify potential candidates as
 people.
 SVM are a data classification method formed by a set of supervised training.
 The aim of this kind of approaches is to produce a model which is able
 to predict classification labels on a test set based only on the descriptors
 of the set and the model.
\end_layout

\begin_layout Standard
The idea behind SVM is that training vectors are mapped on to a bigger dimension
al space in which the data separation, by means of one or many hyperplane,
 is much easier to divide than in the original dimensional space.
\end_layout

\begin_layout Standard
The combination between HOG and SVM leads to a fast detector that depending
 on the situation will perform decently, although it has some main drawbacks
 as its lack of occlusion treatment which does not make this algorithm usable
 when working with crowded spaces.
\end_layout

\begin_layout Standard
The main implementation of Histogram of Oriented Gradient Pedestrian Detector
 is in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Deformable Part Model
\end_layout

\begin_layout Standard
As was mentioned in the previous paragraph one of the main drawbacks when
 working with the simple HOG pedestrian detector is that it describes the
 person model as a hole which leads, inevitably, to the mentioned occlusion
 drawbacks.
 
\end_layout

\begin_layout Subsubsection*
Person Descriptor
\end_layout

\begin_layout Standard
Deformable Part Model tries to solve this problem, among others, by defining
 the model as first, a global coarse template, secondly, several higher
 resolution part templates and finally a spatial model for the location
 of each part.
 This description is the one that can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-detection-obtained with DPM Person model"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/DPM.png
	lyxscale 20
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Detection example obtained with the DPM person model
\end_layout

\end_inset

Detection example obtained with the DPM person model.
 From left to right: Original image + detections, global coarse model, part
 templates and spatial model for each part.
 Image extracted from 
\begin_inset CommandInset citation
LatexCommand cite
key "felzenszwalb2010object"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-detection-obtained with DPM Person model"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Model Generation
\end_layout

\begin_layout Standard
Both global and part templates are modeled with histogram of gradient features
 and the model is built by using an improving over SVM called latent SVM.
 In addition, scores for every detection are obtained by applying a root
 filter on the window plus the sum over parts of the maximum over placements
 of the part filter score on the resulting sub window minus the deformation
 cost.
\end_layout

\begin_layout Subsubsection*
Advantages over alternative PD Approaches
\end_layout

\begin_layout Standard
The use of this detector leads to a set of advantages than when working
 with others simpler approaches.
 In terms of pedestrian occlusion treatment we are able to detect those
 people that have been occluded by something in the scene just by detecting
 some visible part.
 This outperforms other detectors while working with crowded scenes in which
 holistic methods have problems that lead for instance to groups of people
 being detected as a unique detection while DPM is ideally able to separate
 them onto different person instances.
\end_layout

\begin_layout Standard
However, its scanning window approach as well as the part based model lead
 to some computational cost that will increase the time needed to obtain
 detections.
\end_layout

\begin_layout Standard
The main implementation of Deformable Part Model Pedestrian Detector can
 be found in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Aggregated Channel Features Detector 
\end_layout

\begin_layout Standard
The basic idea behind ACF detector is to increase other approaches performance
 by the use of many different channels to describe an input image I.
 
\end_layout

\begin_layout Subsubsection*
Algorithm Stages
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ACF Architecture"

\end_inset

 one can observe the working path of the mentioned detector.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/ACF Detector.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Aggregated Channel Features architecture
\end_layout

\end_inset

Aggregated Channel Features architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:ACF Architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Given an initial image 
\begin_inset Formula $I$
\end_inset

, several channels are computed.
 This channels are named 
\begin_inset Formula $C=\Omega(I)$
\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset

 a set of 10 channels are used to achieve state-of-the-art performance in
 pedestrian detection:
\end_layout

\begin_deeper
\begin_layout Enumerate
Normalized gradient magnitude (1 channel).
\end_layout

\begin_layout Enumerate
Histogram of oriented gradients (6 channels).
\end_layout

\begin_layout Enumerate
LUV color channels (3 channels).
\end_layout

\end_deeper
\begin_layout Enumerate
After the computation, every block of pixels in 
\begin_inset Formula $C$
\end_inset

 is summed and the resulting lower resolution channels are smoothed.
 
\end_layout

\begin_layout Enumerate
After a vectorizing process features are single pixel lookups in the aggregated
 channels.
 Boosting trees are then used to learn this features (pixels) in order to
 distinguish people from the background.
\end_layout

\begin_layout Subsection
Fast Region-Based Convolutional Network
\end_layout

\begin_layout Standard
Fast Region-Based Convolutional Network (Fast R-CNN) is used to perform
 offline pedestrian detection in our proposed system, however, as said in
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 it is not only a pedestrian detector but an algorithm for object detection.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Object-Proposals"

\end_inset

 we mentioned that Fast-RCNN must use objects proposals for its usage.
 In our developed system MCG 
\begin_inset CommandInset citation
LatexCommand cite
key "arbelaez2014multiscale"

\end_inset

 grouping method is used.
\end_layout

\begin_layout Subsubsection*
Algorithm Improvements over R-CNN
\end_layout

\begin_layout Standard
Fast-RCNN method sets its contributions in a several number of innovations
 to improve training and testing speed over its fundamental base R-CNN:
\end_layout

\begin_layout Enumerate
Higher detection quality (mAP).
\end_layout

\begin_layout Enumerate
Training is single-stage, using a multi-task loss.
\end_layout

\begin_layout Enumerate
Training can update all network layers.
\end_layout

\begin_layout Enumerate
No disk storage is required for feature caching.
\end_layout

\begin_layout Subsubsection*
Algorithm Architecture
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast-RCNN-architecture"

\end_inset

 one can observe the general Fast-RCNN architecture.
 The input for the Fast-RCNN network are the entire desired image that one
 wants to process and the set of object proposals.
 Algorithm stages are:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Fast RCNN.png
	lyxscale 10
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Fast-RCNN architecture
\end_layout

\end_inset

Fast-RCNN architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "girshick2015fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Fast-RCNN-architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
The first step in the network is to process the whole image with several
 convolutional and max pooling layers to produce a convolutional feature
 map.
 
\end_layout

\begin_layout Enumerate
Then, for every object proposal present in the image, a region of interest
 (ROI) pooling layer extracts a fixed-length feature vector from the feature
 map.
\end_layout

\begin_layout Enumerate
Each feature vector is after, introduced onto a sequence of fully connected
 (FC) layers that finally diverge onto to output layers.
 
\end_layout

\begin_deeper
\begin_layout Standard
The first one produces probabilities estimates over 
\begin_inset Formula $K$
\end_inset

 object classes.
 
\end_layout

\begin_layout Standard
The second one outputs four real numbers for each of the 
\begin_inset Formula $K$
\end_inset

 object classes.
 This set of 4 values encodes the final bounding box positions for one of
 the objects from the 
\begin_inset Formula $K$
\end_inset

 classes.
\end_layout

\end_deeper
\begin_layout Standard
In this case, both Fast-RCNN detector and MCG object proposal extract are
 implemented within external Matlab libraries and so, should be computed
 offline and introduced externally to the application.
\end_layout

\begin_layout Subsection
PSP-Net
\end_layout

\begin_layout Standard
As we explained in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Contextual-Model-Generation"

\end_inset

 PSP-Net has been trained to detect people as a class.
 In our proposed system we also use this approach by encapsulating connected
 components and pixels labeled as pedestrians.
\end_layout

\begin_layout Section
Multi-Camera Fusion
\end_layout

\begin_layout Standard
Once the semantic model and pedestrian detections are obtained the next
 step is to combine the information from different camera sources and project
 it onto a common plane.
 We call this process multi-camera fusion.
 During this section multi-camera scenarios, generation of reference planes
 and pedestrian fusion is deeply explained.
\end_layout

\begin_layout Subsection
Multi-camera Scenario
\end_layout

\begin_layout Standard
Our system runs on a multi-camera scenario, hence, the scene can be observed
 from different point of views.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration"

\end_inset

 we can observe how the scene is configured with 3 static video-surveillance
 cameras.
 Two of them are placed at the sides of the scene, while one is at the bottom
 part.
 The starting views from the three static cameras are displayed in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Configuration Static.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Analyzing a multi-camera scenario has some advantages.
 Same scene area (as depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 ) can be observed from different points of view.
 This implies that for instance, detections from one camera can be used
 to create new detections in other camera.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 1 Initial View.jpg
	lyxscale 50
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 2 Initial View.jpg
	lyxscale 50
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 3 Initial View.jpg
	lyxscale 50
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial Camera Views
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial Camera Views"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Cenital Plane Homography
\end_layout

\begin_layout Standard
Homography calculation is a pivotal task in our work and is the main base
 for the multi-camera fusion of all the different information.
\end_layout

\begin_layout Standard
The objective is to compute an homography matrix 
\begin_inset Formula $^{\pi_{ref}}H_{F}$
\end_inset

 that relates one camera frame 
\begin_inset Formula $F_{t}$
\end_inset

 at a time 
\begin_inset Formula $t$
\end_inset

, to a so called cenital plane 
\begin_inset Formula $\pi_{ref}$
\end_inset

, i.e.
 a bird-eye representation of the scene.
 
\end_layout

\begin_layout Standard
With this homography matrix we are able to transform every frame pixel 
\begin_inset Formula $F_{t}(x_{1},\,y_{1})$
\end_inset

 to its corresponding position on the cenital plane 
\begin_inset Formula $\pi_{ref}(x_{2},\,y_{2})$
\end_inset

, i.e.
 from 
\begin_inset Formula $2D\,World\rightarrow\text{3D}\,World$
\end_inset

.
 This process is expressed in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Homography Equation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P_{i}=\lambda\cdot{}^{\pi_{ref}}H_{F}\cdot p_{i}\label{eq:Homography Equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
, where 
\begin_inset Formula $\lambda$
\end_inset

 is a scale factor, 
\begin_inset Formula $P_{i}$
\end_inset

 is the subset of points from 
\begin_inset Formula $\pi_{ref}$
\end_inset

 and 
\begin_inset Formula $p_{i}$
\end_inset

 is the subset of points of the 
\begin_inset Formula $F_{t}$
\end_inset

.
\end_layout

\begin_layout Standard
This homography process, apart from transforming frame 
\begin_inset Formula $F_{t}$
\end_inset

 perspective, enables to project pedestrians detections to the cenital plane.
\end_layout

\begin_layout Standard
In order to compute an homography, a relation between at least 4 points
 in each of the two images needs to be computed.
 In our proposed system, these images are a camera frame 
\begin_inset Formula $F_{t}$
\end_inset

 and a computer-generated cenital view plane 
\begin_inset Formula $\pi_{ref}$
\end_inset

.
 This means that there is not a real correspondence for pixels and so, it
 is not possible to use a point descriptor algorithm to extract common points
 between both images.
 User has to manually select the points that represent the same spatial
 place in the images using the Graphical User Interface and then the application
 computes the homography matrix 
\begin_inset Formula $^{\pi_{ref}}H_{F}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Cenital Plan Design
\end_layout

\begin_layout Standard
For the proposed system the image depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cenital plane with camera positions."

\end_inset

 has been used as cenital view plane 
\begin_inset Formula $\pi_{ref}$
\end_inset

.
 The complete creation of the plane is detailed in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Cenital Plane Design Append"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Second Approach Cenital View.png
	lyxscale 30
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cenital plane with camera positions
\begin_inset CommandInset label
LatexCommand label
name "fig:Cenital plane with camera positions."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This plane has been designed in a highly detailed way -e.g.
 see the floor tiles
\begin_inset Formula $-$
\end_inset

 providing enough scene evidences for the homography point selection and
 estimation process.
\end_layout

\begin_layout Subsubsection
Camera Panning
\begin_inset CommandInset label
LatexCommand label
name "subsec:Camera-Panning"

\end_inset


\end_layout

\begin_layout Standard
If cameras are static and pointing to the center of the scene (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

) vision amplitude compared to the complete scenario is quite limited due
 to the low vision range of the cameras.
 They are covering the central part leaving completely unattended the lateral
 parts of the scenario, precluding the analysis of the whole scene.
\end_layout

\begin_layout Standard
A solution for the low vision range problem is to include panning movement
 in the cameras thanks to their PTZ technology.
 This solution is presented in the scene diagram in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration with panning set up"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Panning Configuration.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration with panning setup
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration with panning set up"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Camera Panning Discretization
\end_layout

\begin_layout Standard
Whereas camera panning allows to widen the visualization area, also entails
 a operative problem.
 A specific homography matrix 
\begin_inset Formula $^{\pi_{ref}}H_{Frame_{t}}$
\end_inset

 for each camera position is required for cenital projection, which is an
 impractical solution.
 
\end_layout

\begin_layout Standard
Instead, we propose to sample the panning tour from which 
\begin_inset Formula $N$
\end_inset

 views are selected from the video sequence.
 This leads to the obtention of an homography codebook in where an homography
 matrix 
\begin_inset Formula $^{\pi_{ref}}H_{View}$
\end_inset

 for each of the 
\begin_inset Formula $N$
\end_inset

 camera views is computed.
 This codebook of views and homographies is used for projecting onto the
 cenital plane the whole video sequence.
 Nevertheless, with the codebook, a set of 
\begin_inset Formula $N$
\end_inset

 homographies may be used to project a frame so one needs to be chosen.
 This problem is analyzed in the following section.
\end_layout

\begin_layout Subsubsection
View Selection
\begin_inset CommandInset label
LatexCommand label
name "subsec:View-Selection"

\end_inset


\end_layout

\begin_layout Standard
One have to choose between a set of 
\begin_inset Formula $N$
\end_inset

 codebook homography matrices to project frames and detections onto the
 cenital plane.
 Frame 
\begin_inset Formula $F_{t}$
\end_inset

 should be compared to each of the 
\begin_inset Formula $N$
\end_inset

 views to obtain an spatial correspondence and so, use the correct homography
 matrix from the codebook.
\end_layout

\begin_layout Subsubsection*
Inter-image Comparison
\end_layout

\begin_layout Standard
In order to compare two images a comparison in terms of points of interest
 matching is performed.
 To this aim, we use the AKAZE detector and descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "alcantarilla2011fast"

\end_inset

 (Explained in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:AKAZE append"

\end_inset

).
\end_layout

\begin_layout Enumerate
Codebook 
\begin_inset Formula $N$
\end_inset

 views are described in terms of points of interest using AKAZE.
\end_layout

\begin_layout Enumerate
Frame 
\begin_inset Formula $F_{t}$
\end_inset

 points of interest are also extracted.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $F_{t}$
\end_inset

 points of interest are compared by Brute Force (BF) to each of the 
\begin_inset Formula $N$
\end_inset

 set of points of interest from codebook views.
\end_layout

\begin_layout Enumerate
The comparison that obtains more coincidences is the best spatial correspondent
 and so, the selected view.
\end_layout

\begin_layout Subsubsection*
Intermediate Homography
\end_layout

\begin_layout Standard
If frame 
\begin_inset Formula $F_{t}$
\end_inset

 is not spatially positioned as the correspondent view, the selected homography
 matrix from the codebook does not project the points correctly.
 This problem can be solved by automatically calculating the homography
 between both images (Frame 
\begin_inset Formula $F_{t}$
\end_inset

 and codebook view) using the computed points of interest in the Inter-image
 Comparison section.
 This means that now, to project detections from the current frame 
\begin_inset Formula $F_{t}$
\end_inset

 to the cenital plane 
\begin_inset Formula $\pi_{ref}$
\end_inset

 two homographies are used.
 
\end_layout

\begin_layout Enumerate
Perspective of the original frame 
\begin_inset Formula $F_{t}$
\end_inset

 is changed to the perspective of the selected view in the codebook via
 
\begin_inset Formula $^{View}H_{Frame}$
\end_inset

.
 This process ensures that the used perspective is the same one as the one
 that was used previously to compute the homography.
 
\end_layout

\begin_layout Enumerate
Finally, the frame and its new perspective 
\begin_inset Formula $\tilde{F}_{t}$
\end_inset

 are projected to the cenital plane by means of codebook homography of the
 selected view 
\begin_inset Formula $^{\pi_{ref}}H_{View}$
\end_inset

.
 
\end_layout

\begin_layout Standard
This process is done identically for RGB, semantic frames or even pedestrian
 detections.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:View selection process and homography between views computation"

\end_inset

 graphically depicts Inter-Image Comparison and Intermediate Homography
 processes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/View Selection Process.png
	lyxscale 10
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
View selection process and homography calculation
\end_layout

\end_inset

View selection process and homography between views computation exemplified
 by the projection of the semantic map of a non-sampled frame 
\begin_inset CommandInset label
LatexCommand label
name "fig:View selection process and homography between views computation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Is essential to say, that, as we have to compare the frame 
\begin_inset Formula $F_{t}$
\end_inset

 with all of the 
\begin_inset Formula $N$
\end_inset

 views from the codebook, there is a trade off between accuracy and computationa
l time.
 More sampled views means a better space discretization and more overlapping
 between them and hence, a lower reprojection error in the Intermediate
 Homography step.
 However, the computational time increases exponentially due to comparison
 between frame 
\begin_inset Formula $F_{t}$
\end_inset

 and the codebook views.
 Number of views 
\begin_inset Formula $N$
\end_inset

 has to be selected so it correctly represents the space and it keeps the
 computational time relatively low.
 In our proposed system we have choose 
\begin_inset Formula $N=9$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Video Sequence Synchronization
\end_layout

\begin_layout Standard
One of the main issues when dealing with multi camera systems, and specially,
 those that combine information, is that video sequences should be temporally
 synchronized.
 The same frame number 
\begin_inset Formula $F$
\end_inset

 should represent the same exact moment 
\begin_inset Formula $t$
\end_inset

 in all the cameras so combination and fusion of the information is possible.
\end_layout

\begin_layout Standard
We have synchronized the processed videos using a called clapperboard technique.
 This technique aims to synchronize using concrete events that can be observed
 from all the cameras at the same time.
 By this, we are able to set a synchronize starting point from where the
 following video frames are temporally aligned.
\end_layout

\begin_layout Subsection
Reference Planes and Semantic Fusion
\end_layout

\begin_layout Standard
During this Section our proposed method to create RGB and semantic reference
 planes combining information from the cameras is explained.
 Besides, the computation of semantic shared areas between cameras is analyzed.
\end_layout

\begin_layout Subsubsection
Semantic and RGB Reference Plane Generation
\end_layout

\begin_layout Standard
Following the process scheme of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:View selection process and homography between views computation"

\end_inset

 one can project any video frame onto the cenital perspective.
 By this, a set of projected RGB and semantic frames for each camera are
 obtained (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGB-and-semantic projected"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame207 RGB.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Projected 
\begin_inset Formula $Frame_{t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame1169 RGB.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Projected 
\begin_inset Formula $Frame_{t+\Delta t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame207 Semantic.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Projected 
\begin_inset Formula $Frame_{t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame1169 Semantic.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Projected 
\begin_inset Formula $Frame_{t+\Delta t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB and semantic projected frames 
\begin_inset CommandInset label
LatexCommand label
name "fig:RGB-and-semantic projected"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once all the video frames are projected a temporal average is applied through
 a median filter for each camera.
 A reference plane 
\begin_inset Formula $\pi_{ref-C}$
\end_inset

 enclosed in the ground plane is created by temporally fusing all the projected
 frames from the same camera 
\begin_inset Formula $C$
\end_inset

.
 Given a set of pixels 
\begin_inset Formula $P(x_{0},\,y_{0},\,N)$
\end_inset

 from the same spatial position 
\begin_inset Formula $(x_{0},y_{0})$
\end_inset

 from 
\begin_inset Formula $N$
\end_inset

 different projected images one can define median filter as in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Median Temporal Average"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(\widetilde{x_{0}},\:\widetilde{y_{0}})=\begin{cases}
P_{(N+1)/2} & if\,N\,is\,odd\\
\frac{1}{2}(P_{N/2}+P_{1+N/2}) & if\,N\,is\,even
\end{cases}\label{eq:Median Temporal Average}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
, where 
\begin_inset Formula $P_{N}$
\end_inset

 is the 
\begin_inset Formula $N^{th}$
\end_inset

 order statistics of sorted set 
\begin_inset Formula $P$
\end_inset

.
\end_layout

\begin_layout Standard
Obtained reference frames 
\begin_inset Formula $\pi_{ref-C}$
\end_inset

 from the temporal averaging for RGB can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGB Median Average"

\end_inset

 and for semantic information in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Semantic Median Average"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB1Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1.
 
\begin_inset Formula $^{RGB}\pi_{ref-1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB2Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2.
 
\begin_inset Formula $^{RGB}\pi_{ref-2}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB3Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3.
\begin_inset Formula $^{RGB}\pi_{ref-3}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
RGB Reference Planes
\end_layout

\end_inset

RGB Reference Planes 
\begin_inset Formula $^{RGB}\pi_{ref-C}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:RGB Median Average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem1Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1 
\begin_inset Formula $^{Sem}\pi_{ref-1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem2Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2 
\begin_inset Formula $^{Sem}\pi_{ref-2}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem3Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3 
\begin_inset Formula $^{Sem}\pi_{ref-3}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Semantic Median Average
\end_layout

\end_inset

Semantic Median Average 
\begin_inset Formula $^{Sem}\pi_{ref-C}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Semantic Median Average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Semantic Fusion in the Reference Plane
\begin_inset CommandInset label
LatexCommand label
name "subsec:Semantic-Fusion-in the Reference Plane"

\end_inset


\end_layout

\begin_layout Standard
Once semantic cenital maps 
\begin_inset Formula $^{Sem}\pi_{ref-C}$
\end_inset

 have been extracted, semantic fusion between different camera reference
 planes can be performed.
 The main objective is to obtain common semantic areas, i.e.
 pixels with the same label within two or more cameras.
 These pixels hypothetically have a higher probability to be that label
 as they have been equally detected in two cameras.
 This information may be used to constrain pedestrians detections.
 As the reference plane is obtained by the homography matrix 
\begin_inset Formula $^{View}H_{F}\cdot^{\pi_{ref}}H_{View}$
\end_inset

, we have only considered floor label pixels to create the common areas.
 Other labels may have projection errors for not being enclosed in the ground
 plane.
 Process to extract shared areas is:
\end_layout

\begin_layout Enumerate
Semantic information coming from pairs of cameras is combined to create
 three common floor semantic areas (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas between pair of cameras."

\end_inset

).
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic12.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cam 1 and 2.
 
\begin_inset Formula $^{Sem}\pi_{ref-1}\land{}^{Sem}\pi_{ref-2}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic23.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cam 2 and 3.
 
\begin_inset Formula $^{Sem}\pi_{ref-2}\land{}^{Sem}\pi_{ref-3}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic13.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cam 3 and 1.
 
\begin_inset Formula $^{Sem}\pi_{ref-3}\land{}^{Sem}\pi_{ref-1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Common semantic areas between pair of cameras
\begin_inset CommandInset label
LatexCommand label
name "fig:Common semantic areas between pair of cameras."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Using common semantic between pair of cameras one can extract common floor
 areas for the three cameras at the same time.
 Those areas have been detected as the same label in three cameras so hypothetic
ally, its probability to be floor class is even higher.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas for all the cameras"

\end_inset

 depicts common floor areas between all the cameras.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemanticAllCameras.png
	lyxscale 20
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Common semantic areas for all the cameras
\end_layout

\end_inset

Common semantic areas for all the cameras 
\begin_inset Formula $^{Sem}\pi_{ref-1}\land{}^{Sem}\pi_{ref-2}\land{}^{Sem}\pi_{ref-3}$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Common semantic areas for all the cameras"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Due to the multi camera setup and some scene occlusions the common area
 shared by all the cameras represents a really small area of the room.
\end_layout

\begin_layout Subsection
Pedestrian Reprojection 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-reprojection"

\end_inset


\end_layout

\begin_layout Standard
During this Section we explain how pedestrian detections are reprojected
 from its original camera to other frames.
\end_layout

\begin_layout Subsubsection
Cylinder Estimation
\end_layout

\begin_layout Standard
Detected bounding boxes on one camera frame, due to camera perspective,
 do not correspond spatially with the exact position of the detected object.
 This detection error should be corrected before projecting blobs either
 to another camera instance or to the common cenital plane.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "miguelez2016Deteccion"

\end_inset

 a cylinder estimation technique is proposed to solve this problem.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for camera instances projections"

\end_inset

 graphically represents 
\begin_inset CommandInset citation
LatexCommand cite
key "miguelez2016Deteccion"

\end_inset

 solution for bounding box transference between cameras.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cylinder estimation between cameras.png
	lyxscale 20
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Cylinder estimation for camera instance projections
\end_layout

\end_inset

Cylinder estimation for camera instance projections.
 Extracted from 
\begin_inset CommandInset citation
LatexCommand cite
key "miguelez2016Deteccion"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for camera instances projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
When the blue bounding boxes are projected from image (a) to image (c) they
 are not correctly placed on the pedestrian.
 The solution is to compute the cylinder that embraces the square whose
 side is the blue bounding box (b).
 Once the cylinder is estimated, the person will ideally be in the middle
 of the cylinder.
 
\end_layout

\begin_layout Standard
Cylinder estimation has been integrated in the proposed system to correct
 PD errors.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for cenital view projections"

\end_inset

 represents the method but in this case, applied to the case when the bounding
 box is not projected to another camera frame but to the cenital plane 
\begin_inset Formula $\pi_{ref}$
\end_inset

.
 The projection of the bounding box (green line) is not in the same position
 as the center of the cylinder (end of the purple line).
 The center of the cylinder corresponds to a more approximate position of
 the detected person.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 85text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cylinder estimation for cenital view projections
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for cenital view projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Pedestrian Reprojection Between Cameras
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-Reprojection between cameras"

\end_inset


\end_layout

\begin_layout Standard
When pedestrian detections have been projected onto the cenital plane they
 can be reprojected from the cenital plane back to other camera frames,
 for instance, pedestrian detections from Cameras 2 and 3 may be reprojected
 onto Camera 1.
 This process is done when cameras are observing the same spatial area.
 Ideally, if the three cameras detect the same person, reprojection could
 not be necessary but, if one of them misses the detection, reprojection
 method may lead to avoid that miss detection.
 Steps to fuse detections between cameras are:
\end_layout

\begin_layout Enumerate
Detected pedestrians in all the cameras are projected to the cenital plane
 
\begin_inset Formula $\pi_{ref}$
\end_inset

 by the process from Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:View-Selection"

\end_inset

.
\end_layout

\begin_layout Enumerate
Projected blobs from one camera are transferred to different cameras as
 others detector blobs.
\end_layout

\begin_layout Enumerate
Reprojected blobs are finally created by the use of a fixed aspect ratio
 to, using the width of the blob obtain the height (which was lost due to
 the cenital projection).
\end_layout

\begin_layout Enumerate
Non-Maximum Suppression (NMS) is applied in frame 
\begin_inset Formula $F_{t}$
\end_inset

 to join original detections from its camera and reprojections.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrain reprojection into other cameras"

\end_inset

 represents an example of reprojection.
 If two white circles are displayed, it means that two bounding boxes from
 other cameras have been reprojected.
 However, if only two bounding boxes are drawn means that one reprojection
 has fused with the original detection while the other is not accurate enough
 to do it.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Multi Camera Reprojection/Cam 1.png
	lyxscale 20
	width 49text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1 + Reprojections from 2 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Multi Camera Reprojection/Cam 2.png
	lyxscale 20
	width 49text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2 + Reprojections from 1 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Multi Camera Reprojection/Cam 3.png
	lyxscale 20
	width 49text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3 + Reprojections from 1 and 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Pedestrian detection reprojection
\end_layout

\end_inset

Pedestrian detection reprojection.
 Original and reprojected bounding boxes.
 White circles correspond to reprojected detections from others camera.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrain reprojection into other cameras"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Semantic Filtering
\begin_inset CommandInset label
LatexCommand label
name "sec:Semantic-Filtering"

\end_inset


\end_layout

\begin_layout Standard
Semantic constraining is used in the proposed system to filter false positives
 detections in not plausible semantic scene areas.
 Generally people are always walking on paths so, floor areas may be used
 to constrain pedestrian detections to this hypothesis.
 In our system, common floor areas between pairs of cameras are used instead
 of shared areas between the three cameras (see Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Semantic-Fusion-in the Reference Plane"

\end_inset

) .
 This choice is based on the small common floor area between the three cameras
 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas for all the cameras"

\end_inset

) in comparison to the complete scene.
\end_layout

\begin_layout Standard
The constraint idea is that a bounding box coming from a camera 
\begin_inset Formula $C_{1}$
\end_inset

 is assumed correctly detected if the center of its related cylinder is
 on the common floor between 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 and also between 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{3}$
\end_inset

.
 If that condition is not fulfilled that detection is suppressed.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrain semantic constraining"

\end_inset

 represents different constraining frame examples.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Semantic Constraining/Cam 1.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Semantic Constraining/Cam 2.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Semantic Constraining/Cam 3.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Pedestrian semantic constraining examples
\end_layout

\end_inset

Pedestrian semantic constraining.
 Blobs which projection does not fall on the common floor between two cameras
 are just displayed by a circle and text label.
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrain semantic constraining"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Statistical Usage Data
\end_layout

\begin_layout Standard
Along this Section we explain the proposed system to extract statistical
 usage data frame by frame given a sequence.
 The aim is to obtain one graph per selected class that measures the amount
 of people at a moment 
\begin_inset Formula $t$
\end_inset

 of time in the determined semantic area during the sequence.
\end_layout

\begin_layout Subsection
Statistical Semantic Map Generation
\end_layout

\begin_layout Standard
For this task, rather than creating common areas between cameras as done
 for the PD constraining, information for all the cameras have been joined
 without strictly being shared.
 This have been done to preserve as much semantic information as possible
 and so, have bigger floor or door areas than if we were more restrictive.
\end_layout

\begin_layout Standard
The result from this step is a unique map that represents the hole spatial
 area (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Statistical Semantic Map"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Statistical Map.png
	lyxscale 20
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Statistical Map Legend.png
	lyxscale 10
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Statistical semantic map
\begin_inset CommandInset label
LatexCommand label
name "fig:Statistical Semantic Map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Usage Curves and Paths
\end_layout

\begin_layout Standard
As one can observe we have decided to only take onto account for this process
 floor, doors and chairs, which are the most accurate detections from the
 semantic segmentation.
 Following the same principle as in the previous Section, pedestrians are
 projected on to the statistical semantic map and so one can be able to
 know how many people are in each of the designed areas.
 
\end_layout

\begin_layout Standard
In addition, one can divide Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cenital plane with camera positions."

\end_inset

 into different subregions of fixed size and extract in which of them pedestrian
s density is higher.
\end_layout

\begin_layout Section
Gaussian Representation of Bounding Boxes
\end_layout

\begin_layout Standard
Only for visualization purposes another representation method has been included.
 In the simple cylinder representation pedestrian are represented with two
 perpendicular lines, however, this representation lacks of detections score
 information.
 To solve this issue we propose a new representation method based on a Gaussian
 function placed at the middle of the estimated cylinder.
\end_layout

\begin_layout Standard
Every pedestrian detection is so, represented as a Gaussian function of
 the form described in Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Gaussian Function Equation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x,y)=Aexp\left(-\left(\frac{(x-x_{0}){}^{2}}{2\sigma_{x}^{2}}+\frac{(y-y_{0}){}^{2}}{2\sigma_{y}^{2}}\right)\right)\label{eq:Gaussian Function Equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case 
\begin_inset Formula $A$
\end_inset

 is the amplitude which typically is set to 
\begin_inset Formula $A=1$
\end_inset

, 
\begin_inset Formula $x_{0},\:y_{0}$
\end_inset

 represent the mean of the gaussian which, in our case, is the center of
 the estimated bounding box cylinder and 
\begin_inset Formula $\sigma_{x},\sigma_{y}$
\end_inset

 which is the standard deviation and in our case represents the accuracy
 of the detection.
 
\end_layout

\begin_layout Standard
In order to relate a detection score 
\begin_inset Formula $S_{n}$
\end_inset

 with a determinate standard deviation a simple rule explained in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Score2Deviation"

\end_inset

 is proposed.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma_{x}=\sigma_{y}=(S_{n}\cdot10)+5\label{eq:Score2Deviation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This Equation has been adapted to the size of the used cenital plane.
 Detections with higher scores are represented as a narrow gaussian function
 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:High-score-detections"

\end_inset

) while lower scores lead to wide gaussians (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Low-score-detections"

\end_inset

).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Gaussian Representation 2.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
High score detection.
 Frame 86
\begin_inset CommandInset label
LatexCommand label
name "fig:High-score-detections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Gaussian Representation 1.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Low score detections.
 Frame 87
\begin_inset CommandInset label
LatexCommand label
name "fig:Low-score-detections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Gaussian representation examples
\begin_inset CommandInset label
LatexCommand label
name "fig:Gaussian-representation-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
