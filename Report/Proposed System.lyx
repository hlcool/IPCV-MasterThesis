#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle plain
\bullet 1 2 6 -1
\bullet 2 2 12 -1
\bullet 3 1 25 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Proposed System
\begin_inset CommandInset label
LatexCommand label
name "chap:Proposed-System"

\end_inset


\end_layout

\begin_layout Section
Contextual Model Generation
\end_layout

\begin_layout Standard
One of the main objectives in the scope of this work is to segment frames
 into different semantic areas.
 The relative position in the scene for elements such as doors, walls, paths,
 columns and windows for instance will be necessary to achieve further objective
s.
 For this task we will use an algorithm presented in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

, PSP-Net 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

.
 The goal for this algorithm is is to assign each pixel in the image a category
 label and predict the label, location, as well as shape for each element.
 
\end_layout

\begin_layout Standard
This process is based on a deep convolutional neural network (CNN) called
 pyramid scene parsing network (PSPNet) which is designed to improve performance
 for open-vocabulary object and stuff identification in complex scene parsing.
 The structure of the network can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the proposed PSPNet
\begin_inset CommandInset label
LatexCommand label
name "fig:Overview-of-the proposed PSPNet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Given the image in (a) the first step is to process it through a pre trained
 CNN called ResNet 
\begin_inset CommandInset citation
LatexCommand cite
key "he2016deep"

\end_inset

 to get the feature map (b) of the last convolutional layer.
 The final feature map in this step is 
\begin_inset Formula $1/8$
\end_inset

 of the size of the input image.
 
\end_layout

\begin_layout Standard
The second step is to apply what is called the Pyramid Pooling Module (c).
 The main objective of the module is to collect a few levels of information,
 more representative than global pooling.
 It separates the feature map into different sub-regions and forms pooled
 representation for different locations.
 Here a set of pooling, convolutional and upsampling layers are applied
 that will harvest different sub-region representation in 
\begin_inset Formula $N$
\end_inset

 different scales.
 
\end_layout

\begin_layout Standard
After the bilinear interpolation upsampling, concatenation layers are used
 to form the final feature representation by fusing the feature map extracted
 in (b) and the Pyramid Pooling Module output.
 This final feature carries both local and global context information.
 In the end, the representation is fed into a convolutional layer which
 gets the final per-pixel prediction (d).
\end_layout

\begin_layout Standard

\color blue
\begin_inset CommandInset href
LatexCommand href
name "PSP-Net"
target "https://github.com/hszhao/PSPNet"

\end_inset

 
\color inherit
comes with a set of three different pre trained caffe models for three different
 Datasets.
 The main difference between the model for our scope is the environment
 in which the network has been trained.
\end_layout

\begin_layout Itemize
ADE20K: This data set is the more challenging as it has up to 150 different
 labels in a wide range of scenes.
 The scenes go from interior room places to outdoor scenarios.
\end_layout

\begin_layout Itemize
VOC2012: It contains 20 object categories and one background class from
 diverse indoor scenes.
\end_layout

\begin_layout Itemize
CityScapes: The last dataset defines 19 categories containing both stuff
 and objects.
 All the available sequences have been recorded from a driving car in the
 street.
\end_layout

\begin_layout Standard
So, as one can observe the three different model represent different object
 categories in different spaces.
 In our case we will select the model based on two main reasons:
\end_layout

\begin_layout Enumerate
The model should be trained with indoor scenes.
 This will lead to discard those models that performs in outdoors scenes
 as we would like our work to perform in interior scenarios.
 This will exclude CityScapes dataset from our options as all the classes
 are from outdoor scenes.
\end_layout

\begin_layout Enumerate
From the trained indoor models we have to choose those whose categories
 best fit in our work.
 In this case VOC2012 dataset uses classes such as boat, airplane or table
 which are not interesting for our segmentation.
\end_layout

\begin_layout Standard
With this two reasons taken into account we have selected the model ADE20K
 because it has elements such as walls, floor, person and column.
 However, we consider that most of the 150 label categories will be unused
 in our procedure, so, the number of classes in the model has been reduced
 to the 21 classes of our interest so we only obtain scores for those objects
 that we want.
 This class limitations will lead above all in a considerably hard drive
 space saving.
 In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset

 can be observed the final 21 classes that have been used.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
wall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
building
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
floor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ceiling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
road
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
window pane
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
person
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
chair
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
seat
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
desk
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
lamp
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
column
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
counter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
path
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
screen door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairway
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
toilet
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
poster
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
bag
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Final classes list from ADE20K to use in PSP-Net
\begin_inset CommandInset label
LatexCommand label
name "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Pedestrian Detection
\end_layout

\begin_layout Standard
Along this section we will present the pedestrian State of the Art detectors
 that have been selected for our work.
 Some of them have been chosen due to the algorithm simplicity and computational
 cost and other due to its contrasted performance while working in real
 sequences as we have seen in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

.
 However, one of the main reasons of choosing the following three algorithms
 among others in the SoA is the possibility of either having the original
 source code or the practical implementation within an external library.
\end_layout

\begin_layout Subsection
Histogram of Oriented Gradients
\end_layout

\begin_layout Standard
Histogram of Oriented Gradients, i.e.
 HOG, is one of the main used detectors along pedestrian detection field.
 This fact is due to it's extremely simplicity in terms of describer complexity.
 Pedestrians are described as set of HOG, this means that its shape and
 appearance can be described by a set of gradients and intensities organized
 as orientation histograms.
 These histograms will describe intensity distributions from local gradients
 or border directions.
 This descriptor can be observe graphically in Figure 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/HoG.jpg
	lyxscale 60
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
HOG Pedestrian Descriptor
\begin_inset CommandInset label
LatexCommand label
name "fig:HOG-Pedestrian-Descriptor"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once HOG have been used to describe the personal shape, Support Vector Machines
 are used to train a person model and to classify potential candidates as
 people.
 SVM are a data classification method formed by a set of supervised training.
 The aim of this kind of approaches is to produce a model which will be
 able to predict classification labels on a test set based only on the descripto
rs of the set.
\end_layout

\begin_layout Standard
The idea behind SVM is that training vectors will be mapped on to a bigger
 dimensional space in which the data separation by means of one or many
 hyperplane will be much easier than in the original dimensional space.
\end_layout

\begin_layout Standard
The combination between HOG and SVM will lead to a fast detector that depending
 on the situation will perform decently, although it has some main drawbacks
 as its lack of occlusion treatment which will not make this algorithm usable
 when working with crowded spaces.
\end_layout

\begin_layout Standard
The main implementation of Histogram of Oriented Gradient Pedestrian Detector
 is in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Deformable Part Model
\end_layout

\begin_layout Standard
As was mentioned in the previous paragraph one of the main drawbacks when
 working with the simple HOG pedestrian detection is that it describes the
 person model as a hole which will lead to the described drawbacks.
 Deformable Part Model solves this problem, among others, by defining the
 model as a global coarse template, several higher resolution part templates
 and a spatial model for the location of each part.
 This description is the one that can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-detection-obtained with DPM Person model"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/DPM.png
	lyxscale 20
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example detection obtained with the person model.
 From left to right: Original image + detections, global coarse model, part
 templates and spatial model for each part.
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-detection-obtained with DPM Person model"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "felzenszwalb2010object"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Both global and part templates are modeled with histogram of gradient features
 and the model is built by using an improving over SVM called latent SVM.
 In addition, scores for every detection are obtained by applying a root
 filter on the window plus the sum over parts of the maximum over placements
 of that par, of the part filter score on the resulting sub window minus
 the deformation cost.
\end_layout

\begin_layout Standard
The use of this detector will lead to a set of advantages than when working
 with others simpler approaches.
 In terms of pedestrian occlusion treatment we will be able to detect those
 people that have been occluded by something in the scene just by detecting
 some visible part.
 This will outperform other detectors while working with crowded scenes
 in which holistic methods will have problems that will lead for instance
 to groups of people being detected as a unique detection while DPM will
 be able to separate them into different person instances.
\end_layout

\begin_layout Standard
However, its scanning window approach as well as the part based model will
 lead to some computational cost that will increase the time needed to obtain
 detections.
\end_layout

\begin_layout Standard
The main implementation of Deformable Part Model Pedestrian Detector is
 in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Fast Region-Based Convolutional Network
\end_layout

\begin_layout Standard
Fast Region-based Convolutional Network (Fast R-CNN) it will be used to
 detect pedestrian in our proposed system, however, as said in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 it is not only a pedestrian detector but a method for object detection.
 In 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Object-Proposals"

\end_inset

 we mentioned that Fast-RCNN must use objects proposals for its usage.
 In the developed system MCG 
\begin_inset CommandInset citation
LatexCommand cite
key "arbelaez2014multiscale"

\end_inset

 grouping method will be used for this purpose.
\end_layout

\begin_layout Standard
Fast-RCNN method employs set its contributions in a several number of innovation
s to improve training and testing speed over its fundamental base R-CNN:
\end_layout

\begin_layout Enumerate
Higher detection quality (mAP)
\end_layout

\begin_layout Enumerate
Training is single-stage, using a multi-task loss 
\end_layout

\begin_layout Enumerate
Training can update all network layers 
\end_layout

\begin_layout Enumerate
No disk storage is required for feature caching
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast-R-RCNN-architecture"

\end_inset

 one can observe the general Fast R-RCNN architecture.
 The input for the Fast-RCNN network are the entire desired image that one
 wants to process and a set of object proposals.
 The first step in the network is to process the whole image with several
 convolutional and max pooling layers to produce a convolutional feature
 map.
 Then, for every object proposal present in the image, a region of interest
 (ROI) pooling layer extracts a fixed-length feature vector from the feature
 map.
\end_layout

\begin_layout Standard
Each feature vector is after introduced into a sequence of fully connected
 (FC) layers that finally diverge into to output layers.
 The first one produces probabilities estimates over 
\begin_inset Formula $K$
\end_inset

 object classes.
 The second one outputs four real numbers for each of the 
\begin_inset Formula $K$
\end_inset

 object classes.
 This set of 4 values encodes the final bounding-box positions for one of
 the objects from the 
\begin_inset Formula $K$
\end_inset

 classes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Fast RCNN.png
	lyxscale 10
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Fast R-RCNN architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "girshick2015fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Fast-R-RCNN-architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this case, both Fast-RCNN detector and MCG object proposal extract are
 implemented within external Matlab libraries.
\end_layout

\begin_layout Section
Fusion and Filtering
\end_layout

\begin_layout Standard
Once the semantic model and pedestrian detections are implemented the next
 step is to combine the information coming from different sources to in
 a process that we have called fusion and filtering.
\end_layout

\begin_layout Subsection
Multi-camera Scenario
\end_layout

\begin_layout Standard
Our system is developed in a multi-camera scenario.
 This means that we will be able to observe the scene from different point
 of views.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration"

\end_inset

 we can observe how the scene is configured with 3 different video-surveillance
 cameras.
 Two of them are placed at the sides of the scene, while one is at the bottom
 part.
 The starting views from the three cameras is displayed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Configuration Static.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 1 Initial View.jpg
	lyxscale 50
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 2 Initial View.jpg
	lyxscale 50
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 3 Initial View.jpg
	lyxscale 50
	width 30text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial Camera Views
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial Camera Views"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As said in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 working with a multi-camera scenario has some advantages.
 As one can see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 we can observe the same scene area from three different point of views,
 which for example means, that detections from one camera can be used to
 detect in other camera or even refine the available detections.
 
\end_layout

\begin_layout Subsubsection
Cenital Plane Homography
\end_layout

\begin_layout Standard
Homography calculation is a pivotal task in our work and the main base for
 the fusion of all the different information coming from the three cameras.
 The objective is to compute an homography matrix that will relate a camera
 frame to the cenital plane.
 With this homography matrix we will be able to transform every frame pixel
 to its correspondent position in the cenital plane, i.e.
 it will transform the frame as if it were being viewed from the top.
\end_layout

\begin_layout Standard
In order to compute an homography, a relation between at least 4 points
 in each of the two images should be computed.
 In this case we are trying to create a relationship between a real camera
 frame, and a cenital view plane created by computer.
 This means that there will not be a real correspondent for pixels and so
 it is not possible to use a point descriptor to extract points in both
 images.
 User has to manually select the points that represent the same spatial
 place in both images using the Graphical User Interface and then the algorithm
 will compute the matrix.
\end_layout

\begin_layout Subsubsection
Cenital Plan Design
\end_layout

\begin_layout Standard
As said in the previous paragraph, one of the main objectives of the work
 is to project the extracted detections from the three cameras, i.e.
 pedestrian and semantic, into a common plane for all of them.
 To achieve this goal a cenital plane from the scene is needed.
 The first approach for the cenital plane can be observe in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:First cenital plane approach"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
First cenital plane approach
\begin_inset CommandInset label
LatexCommand label
name "fig:First cenital plane approach"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can observe the first approach of the cenital plane lacks of details
 from the scene.
 To compute an homography between the camera frame and the cenital plane
 we should be able to identify the same scene points in both images in the
 floor plane.
 This means that the cenital plane should have enough detail so the point
 selection is done correctly and the homography is correctly computed.
\end_layout

\begin_layout Standard
For this reason another cenital plane has been compute starting from zero.
 In this new approach the scene has been correctly measure by hand and the
 plane has been done with real measures and high floor detail.
\end_layout

\begin_layout Standard
For correctly drawing the plane AutoCAD 2017 software has been used.
 The second plane approach can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Second cenital plane approach with measures"

\end_inset

 with all the annotated measures and in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Second cenital plane approach with camera positions"

\end_inset

 with the camera positions.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Second cenital plane approach with measures
\begin_inset CommandInset label
LatexCommand label
name "fig:Second cenital plane approach with measures"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Second cenital plane approach with camera positions
\begin_inset CommandInset label
LatexCommand label
name "fig:Second cenital plane approach with camera positions"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now the cenital plane has much more detail that in the first approach and
 as we will se later, the homography selection points will be better selected
 and so all the projections will be more accurate.
\end_layout

\begin_layout Subsubsection
Camera Panning
\end_layout

\begin_layout Standard
When cameras are static and pointing to the center of the scene as in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 one can easily see that the vision range compared to the hole scenario
 is quite limited due to the low vision range of the cameras.
 The cameras are covering the middle part leaving unattended the lateral
 parts of the scenario.
 The solution to this problem is to include panning movement in all the
 cameras from left to right so cameras view range is increased and no longer
 focus in the middle of the scene as before.
 This will lead also that the common areas that the cameras will share are
 increased.
 This solution is presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration with panning set up"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 50
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration with panning set up
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration with panning set up"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Camera Panning Discretization
\end_layout

\begin_layout Standard
Before including camera panning in the set up, homographies where calculated,
 at the beginning, for only one video frame per camera.
 This means that every camera only had one homography matrix as all its
 sequence was static and all its video frames where projected using the
 same matrix.
 However, when including video panning this solution is no longer valid
 as the view is constantly changing.
 The ideal solution for this problem would be having one homography matrix
 
\begin_inset Formula $H$
\end_inset

 per video frame.
 As the homographies are calculated by the user selecting the points this
 solution is evidentially impossible in term of usability.
 We propose discretization of the panning tour in which 
\begin_inset Formula $N$
\end_inset

 vies are selected from the video sequence.
 This will lead to an homography codebook in where the user will compute
 a homography matrix 
\begin_inset Formula $H$
\end_inset

 for each of the 
\begin_inset Formula $N$
\end_inset

 camera views.
\end_layout

\begin_layout Subsubsection
View Selection
\end_layout

\begin_layout Standard
Due to the creation of the homography codebook we now have to choose between
 a set of 
\begin_inset Formula $N$
\end_inset

 homography matrices to project detections into the cenital plane.
 The actual analyzed frame should be compared to each of the 
\begin_inset Formula $N$
\end_inset

 views to obtain an spatial correspondence and so, use the correct homography
 matrix.
\end_layout

\begin_layout Standard
In order to compare two images we have done comparison between interest
 points.
 AKAZE detector and descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "alcantarilla2011fast"

\end_inset

 has been used for this task.
\end_layout

\begin_layout Standard
AKAZE will detect and describe points of interest in any image and then
 by a brute force comparator we will extract how many coincidence we have
 between a pair of images.
 It is essential to say, that, as we have to compare the current frame with
 all of the 
\begin_inset Formula $N$
\end_inset

 vies we will have a trade off.
 More view will mean a better space discretization however the computational
 time will increase exponentially, this means that we have to choose the
 number of views 
\begin_inset Formula $N$
\end_inset

 so it represents correctly the space and keeps the computational time low.
\end_layout

\begin_layout Standard
As we said before, the homographies are calculated for each of the views,
 this means, that if a frame is not positioned exactly as a view the homography
 matrix will not project the points correctly, there will be a small error.
 Taking advantage of the calculation of common interest point between the
 frame and its correspondent view we can solve this problem by calculating
 automatically the homography between both images.
 This means that now, to project detections from the current frame to the
 cenital plane, first we will change the perspective of it to the perspective
 of the view.
 This way we will ensure that the used perspective is the same one as the
 one that was used to compute the homography.
\end_layout

\begin_layout Subsubsection
Video Sequence Synchronization
\end_layout

\begin_layout Standard
Synchronization between different camera videos by using concrete common
 events in the sequences.
\end_layout

\begin_layout Subsection
Pedestrian Filtering
\end_layout

\begin_layout Subsubsection
Cylinder Estimation
\end_layout

\begin_layout Standard
In [Paper Rafael Nieto y Alejandro Miguélez (TFG)] a cylinder estimation
 technique is proposed.
 The detected bounding boxes on the camera frame do not correspond spatially
 with the position of the detected object due to the camera perspective.
 If this detection error is not corrected when the bounding boxes are projected
 either to another camera instance or to the common cenital plane there
 will be a distance between the bounding box and the real object.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for camera instances projections"

\end_inset

 shows the case for bounding box transference between cameras and Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for cenital view projections"

\end_inset

 for the cenital frame.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cylinder estimation between cameras.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cylinder estimation for camera instance projections
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for camera instances projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cylinder estimation in cenital.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cylinder estimation for cenital view projections
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for cenital view projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The final representation of the projected bounding boxes into the cenital
 plane using this approach will be two perpendicular lines.
 The first one will represent the projection of the bounding box and the
 second one will represent the real position of the object into the plane.
\end_layout

\begin_layout Standard

\color red
Poner aqui o en resultados?
\end_layout

\begin_layout Subsubsection
Gaussian Bounding Box Representation
\end_layout

\begin_layout Standard
Using also the cylinder representation one can change the way bounding boxes
 are represented in the cenital plane.
 As said before, in the simple cylinder representation pedestrian will be
 represented with two perpendicular lines, however the middle point from
 the cylinder can be used to present a gaussian.
\end_layout

\begin_layout Standard
In this case we will project every pedestrian detection as a Gaussian function
 of the form described in Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Gaussian Function Equation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x,y)=Aexp\left(-\left(\frac{(x-x_{0}){}^{2}}{2\sigma_{x}^{2}}+\frac{(y-y_{0}){}^{2}}{2\sigma_{y}^{2}}\right)\right)\label{eq:Gaussian Function Equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case 
\begin_inset Formula $A$
\end_inset

 will be the amplitude, 
\begin_inset Formula $x_{0},y_{0}$
\end_inset

 the mean of the gaussian which, in our case will be the center of the blob
 and 
\begin_inset Formula $\sigma_{x},\sigma_{y}$
\end_inset

 which is the standard deviation and in our case will represent the accuracy
 of the detection.
 This means that detections with higher scores will be represented as a
 narrow gaussian function while lower scores will lead to wide gaussians
 that will represent a bigger area in where one can find that person.
\end_layout

\begin_layout Standard

\color red
Aqui o en resultados?
\end_layout

\begin_layout Section
Statistical Usage Data
\end_layout

\begin_layout Standard
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––
––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––
––––––––––––––––––––––––––––––––––––––––––––––––––––––––
\end_layout

\begin_layout Section
Induced Plane Homography
\end_layout

\begin_layout Standard
Promediado de semanticas projectadas en la imagen cenital para las 3 camaras.
\end_layout

\begin_layout Standard
Puntos comunes entre promedidados de semanticas para pares de camaras.
\end_layout

\begin_layout Standard
Puntos comunes entre los promedidados de las tres camaras -> Primera hipotesis
 de puntos pertenencientes al suelo.
 Extraccion de scores para esos puntos.
\end_layout

\begin_layout Standard
Generacion de planos paralelos al cenital para proyectar nuevos puntos comunes
 a otra altura.
\end_layout

\end_body
\end_document
