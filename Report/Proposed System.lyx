#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle plain
\bullet 1 2 6 -1
\bullet 2 2 12 -1
\bullet 3 1 25 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Proposed System
\begin_inset CommandInset label
LatexCommand label
name "chap:Proposed-System"

\end_inset


\end_layout

\begin_layout Standard
During this Chapter our proposed system is analyzed.
 We start from the contextual model generation and pedestrian detectors.
 Then, the fusion of all the obtained elements in a multi camera system
 with semantic constraints is described.
 Finally, a case of example which generates statistical usage data from
 semantic areas is proposed.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Proposed-system-flowchart"

\end_inset

 depicts the flowchart of the modular proposed method.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/System FlowChart.svg
	lyxscale 30
	width 100text%
	height 100theight%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Flowchart of the proposed method
\begin_inset CommandInset label
LatexCommand label
name "fig:Proposed-system-flowchart"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Contextual Model Generation
\begin_inset CommandInset label
LatexCommand label
name "sec:Contextual-Model-Generation"

\end_inset


\end_layout

\begin_layout Standard
One of the main objectives in the scope of this work is to perform semantic
 segmentation, which 
\begin_inset Formula $-$
\end_inset

as explained in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset


\begin_inset Formula $-$
\end_inset

targets to divide one frame into different semantic areas.
 The relative position in the scene for elements such as doors, walls, paths,
 and columns is required to achieve further objectives such as multi camera
 pedestrian constraint and statistical data extraction.
 
\end_layout

\begin_layout Standard
For this complex task the algorithm PSP-Net 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

 presented in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 is used.
 We choose PSP-Net because at the moment of this work was the one with available
 code achieving the best results (see Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Cityscapes Dataset Challenge Results"

\end_inset

).
 The goal of this algorithm is is to assign each pixel in the image a category
 label.
\end_layout

\begin_layout Subsection
Pyramid Scene Parsing Network
\end_layout

\begin_layout Standard
It uses a deep Convolutional Neural Network (CNN) called Pyramid Scene Parsing
 Network (PSPNet).
 This network is designed to improve performance for open-vocabulary object
 identification in complex scene parsing.
 The structure of the network is represented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/PSP-Net.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of the proposed PSPNet
\begin_inset CommandInset label
LatexCommand label
name "fig:Overview-of-the proposed PSPNet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Algorithm Stages
\end_layout

\begin_layout Standard
The algorithm is divided into different stages:
\end_layout

\begin_layout Enumerate
Image in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(a) is processed it through a pre trained CNN called ResNet 
\begin_inset CommandInset citation
LatexCommand cite
key "he2016deep"

\end_inset

.
 The objective is to get the full feature map 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(b) of the last convolutional layer.
 The final feature map in this step is 
\begin_inset Formula $1/8$
\end_inset

 of the size of the input image.
 
\end_layout

\begin_layout Enumerate
Apply the main contribution of 
\begin_inset CommandInset citation
LatexCommand cite
key "zhao2016pyramid"

\end_inset

, called the Pyramid Pooling Module 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(c).
 The main objective of the module is to collect a few levels of information,
 much more representative than global pooling.
 It separates the feature map into different sub-regions and forms pooled
 representations for different locations.
 Here a set of pooling, convolutional and upsampling layers are applied
 to harvest different sub-region representation in 
\begin_inset Formula $N$
\end_inset

 different scales.
 
\end_layout

\begin_layout Enumerate
Concatenation layers are used to form the final feature representation by
 fusing the feature map extracted in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(b) and the Pyramid Pooling Module output.
 This final feature carries both local and global context information.
 
\end_layout

\begin_layout Enumerate
The representation is fed into a convolutional layer which gets the final
 per-pixel prediction 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Overview-of-the proposed PSPNet"

\end_inset

.(d).
\end_layout

\begin_layout Subsubsection*
Semantic Segmentation Particularization
\end_layout

\begin_layout Standard

\color blue
\begin_inset CommandInset href
LatexCommand href
name "PSP-Net"
target "https://github.com/hszhao/PSPNet"

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout

\color blue
https://github.com/hszhao/PSPNet
\end_layout

\end_inset

 
\color inherit
comes with a set of three different pre trained 
\color blue

\begin_inset CommandInset href
LatexCommand href
name "Caffe"
target "http://caffe.berkeleyvision.org/"

\end_inset


\shape italic

\begin_inset Foot
status collapsed

\begin_layout Plain Layout

\color blue
http://caffe.berkeleyvision.org/
\end_layout

\end_inset


\shape default
\color inherit
 models for three different datasets.
 The main difference between the models for our scope is the environment
 in which the network has been trained.
\end_layout

\begin_layout Itemize
ADE20K: This dataset is the most challenging as it has up to 150 different
 labels in a wide range of scenes.
 The scenes go from interior room places to outdoor scenarios.
\end_layout

\begin_layout Itemize
VOC2012: It contains 20 object categories and one background class from
 diverse indoor and outdoor scenes.
\end_layout

\begin_layout Itemize
CityScapes: The last dataset defines 19 categories containing both stuff
 and objects.
 All the available sequences have been recorded from a driving car while
 driving in the street.
\end_layout

\begin_layout Subsubsection*
Model Particularization
\end_layout

\begin_layout Standard
As one can observe the three different models represent different object
 categories in different real spaces.
 In our case we select the model based on two main reasons:
\end_layout

\begin_layout Enumerate
The model should have been trained with indoor scenes.
 This leads to discard those models that represent only outdoors scenes
 as we would like our approach to be used in an interior scenario.
 This will exclude CityScapes dataset from our options as all the classes
 and sequences used for training are from outdoor scenes.
\end_layout

\begin_layout Enumerate
From the trained indoor models we have to choose between those whose categories
 best fit in our work.
 In this case VOC2012 dataset uses classes such as boat, airplane or table
 which are not interesting for our segmentation problem and it does not
 have classes such as door or wall which are really important for us.
\end_layout

\begin_layout Subsubsection*
Selected Model
\end_layout

\begin_layout Standard
Considering these two reasons, we have selected the model ADE20K because
 as said, it has elements such as walls, floor, person and column in its
 model.
 
\end_layout

\begin_layout Standard
However, we consider that most of the 150 label categories will be unused
 in our procedure, so, the number of classes from the model has been reduced
 to the 21 classes of our interest.
 Position and scores for those objects are the only ones obtained.
 This class limitation leads above all in a considerably hard drive space
 saving.
 In Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset

 the final 21 selected classes are exposed.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setlength
\backslash
extrarowheight{7pt}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
wall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
building
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
floor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
ceiling
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
road
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
window pane
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
person
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
table
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
chair
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
seat
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
desk
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
lamp
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
column
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
counter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
path
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
screen door
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
stairway
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
toilet
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
poster
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
bag
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Final classes list from ADE20K to use in PSP-Net
\begin_inset CommandInset label
LatexCommand label
name "tab:Final classes list from ADE20K to use in PSP-Net"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Pedestrian Detection
\end_layout

\begin_layout Standard
Along this section pedestrian State of the Art detectors that have been
 integrated in the proposed system are presented.
 Some of them have been chosen due to their efficiency, whereas some have
 been chosen due to its contrasted good performance (see Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

).
\end_layout

\begin_layout Standard
Besides, algorithm source code of all the chosen approaches is available.
\end_layout

\begin_layout Standard
However, one of the main reasons of choosing the following five algorithms
 among others in the SoA is the possibility of either having the original
 source code or the practical implementation within an external OpenCV library.
\end_layout

\begin_layout Subsection
Histogram of Oriented Gradients
\end_layout

\begin_layout Standard
Histogram of Oriented Gradients, i.e.
 HOG, is one of the main used detectors along pedestrian detection field.
 This fact is due to it's extremely simplicity in terms of the descriptor
 complexity.
 
\end_layout

\begin_layout Subsubsection*
Person Descriptor
\end_layout

\begin_layout Standard
Pedestrians are described as set of HOG.
 This means that its shape and appearance can be described by a set of gradients
 and intensities organized as orientation histograms.
 These histograms describe intensity distributions from local gradients
 or border directions.
 This descriptor can be observed graphically in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:HOG-Pedestrian-Descriptor"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/HoG.jpg
	lyxscale 60
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
HOG Pedestrian Descriptor
\begin_inset CommandInset label
LatexCommand label
name "fig:HOG-Pedestrian-Descriptor"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Model Generation
\end_layout

\begin_layout Standard
Once HOG have been used to describe the person shape, Support Vector Machines
 are used to train a person model and to classify potential candidates as
 people.
 SVM are a data classification method formed by a set of supervised training.
 The aim of this kind of approaches is to produce a model which is able
 to predict classification labels on a test set based only on the descriptors
 of the set and the model.
\end_layout

\begin_layout Standard
The idea behind SVM is that training vectors are mapped on to a bigger dimension
al space in which the data separation, by means of one or many hyperplane,
 is much easier to divide than in the original dimensional space.
\end_layout

\begin_layout Standard
The combination between HOG and SVM leads to a fast detector that depending
 on the situation will perform decently, although it has some main drawbacks
 as its lack of occlusion treatment which does not make this algorithm usable
 when working with crowded spaces.
\end_layout

\begin_layout Standard
The main implementation of Histogram of Oriented Gradient Pedestrian Detector
 is in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Deformable Part Model
\end_layout

\begin_layout Standard
As was mentioned in the previous paragraph one of the main drawbacks when
 working with the simple HOG pedestrian detector is that it describes the
 person model as a hole which leads, inevitably, to the mentioned occlusion
 drawbacks.
 
\end_layout

\begin_layout Subsubsection*
Person Descriptor
\end_layout

\begin_layout Standard
Deformable Part Model tries to solve this problem, among others, by defining
 the model as first, a global coarse template, secondly, several higher
 resolution part templates and finally a spatial model for the location
 of each part.
 This description is the one that can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-detection-obtained with DPM Person model"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/DPM.png
	lyxscale 20
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Detection example obtained with the DPM person model
\end_layout

\end_inset

Detection example obtained with the DPM person model.
 From left to right: Original image + detections, global coarse model, part
 templates and spatial model for each part.
 Image extracted from 
\begin_inset CommandInset citation
LatexCommand cite
key "felzenszwalb2010object"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-detection-obtained with DPM Person model"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Model Generation
\end_layout

\begin_layout Standard
Both global and part templates are modeled with histogram of gradient features
 and the model is built by using an improving over SVM called latent SVM.
 In addition, scores for every detection are obtained by applying a root
 filter on the window plus the sum over parts of the maximum over placements
 of the part filter score on the resulting sub window minus the deformation
 cost.
\end_layout

\begin_layout Subsubsection*
Advantages over alternative PD Approaches
\end_layout

\begin_layout Standard
The use of this detector leads to a set of advantages than when working
 with others simpler approaches.
 In terms of pedestrian occlusion treatment we are able to detect those
 people that have been occluded by something in the scene just by detecting
 some visible part.
 This outperforms other detectors while working with crowded scenes in which
 holistic methods have problems that lead for instance to groups of people
 being detected as a unique detection while DPM is ideally able to separate
 them into different person instances.
\end_layout

\begin_layout Standard
However, its scanning window approach as well as the part based model lead
 to some computational cost that will increase the time needed to obtain
 detections.
\end_layout

\begin_layout Standard
The main implementation of Deformable Part Model Pedestrian Detector can
 be found in OpenCV library for C++.
\end_layout

\begin_layout Subsection
Aggregated Channel Features Detector 
\end_layout

\begin_layout Standard
The basic idea behind ACF detector is to increase other approaches performance
 by the use of many different channels to describe an input image I.
 
\end_layout

\begin_layout Subsubsection*
Algorithm Stages
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ACF Architecture"

\end_inset

 one can observe the working path of the mentioned detector.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/ACF Detector.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Aggregated Channel Features architecture
\end_layout

\end_inset

Aggregated Channel Features architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:ACF Architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Given an initial image 
\begin_inset Formula $I$
\end_inset

, several channels are computed.
 This channels are named 
\begin_inset Formula $C=\Omega(I)$
\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "dollar2014fast"

\end_inset

 a set of 10 channels are used to achieve state-of-the-art performance in
 pedestrian detection:
\end_layout

\begin_deeper
\begin_layout Enumerate
Normalized gradient magnitude (1 channel).
\end_layout

\begin_layout Enumerate
Histogram of oriented gradients (6 channels).
\end_layout

\begin_layout Enumerate
LUV color channels (3 channels).
\end_layout

\end_deeper
\begin_layout Enumerate
After the computation, every block of pixels in 
\begin_inset Formula $C$
\end_inset

 is summed and the resulting lower resolution channels are smoothed.
 
\end_layout

\begin_layout Enumerate
After a vectorizing process features are single pixel lookups in the aggregated
 channels.
 Boosting trees are then used to learn this features (pixels) in order to
 distinguish people from the background.
\end_layout

\begin_layout Subsection
Fast Region-Based Convolutional Network
\end_layout

\begin_layout Standard
Fast Region-Based Convolutional Network (Fast R-CNN) is used to perform
 offline pedestrian detection in our proposed system, however, as said in
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:State of the art"

\end_inset

 it is not only a pedestrian detector but an algorithm for object detection.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Object-Proposals"

\end_inset

 we mentioned that Fast-RCNN must use objects proposals for its usage.
 In our developed system MCG 
\begin_inset CommandInset citation
LatexCommand cite
key "arbelaez2014multiscale"

\end_inset

 grouping method is used.
\end_layout

\begin_layout Subsubsection*
Algorithm Improvements over R-CNN
\end_layout

\begin_layout Standard
Fast-RCNN method sets its contributions in a several number of innovations
 to improve training and testing speed over its fundamental base R-CNN:
\end_layout

\begin_layout Enumerate
Higher detection quality (mAP).
\end_layout

\begin_layout Enumerate
Training is single-stage, using a multi-task loss.
\end_layout

\begin_layout Enumerate
Training can update all network layers.
\end_layout

\begin_layout Enumerate
No disk storage is required for feature caching.
\end_layout

\begin_layout Subsubsection*
Algorithm Architecture
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast-RCNN-architecture"

\end_inset

 one can observe the general Fast-RCNN architecture.
 The input for the Fast-RCNN network are the entire desired image that one
 wants to process and the set of object proposals.
 Algorithm stages are:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Fast RCNN.png
	lyxscale 10
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Fast-RCNN architecture
\end_layout

\end_inset

Fast-RCNN architecture.
 
\begin_inset CommandInset citation
LatexCommand cite
key "girshick2015fast"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Fast-RCNN-architecture"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
The first step in the network is to process the whole image with several
 convolutional and max pooling layers to produce a convolutional feature
 map.
 
\end_layout

\begin_layout Enumerate
Then, for every object proposal present in the image, a region of interest
 (ROI) pooling layer extracts a fixed-length feature vector from the feature
 map.
\end_layout

\begin_layout Enumerate
Each feature vector is after, introduced into a sequence of fully connected
 (FC) layers that finally diverge into to output layers.
 
\end_layout

\begin_deeper
\begin_layout Standard
The first one produces probabilities estimates over 
\begin_inset Formula $K$
\end_inset

 object classes.
 
\end_layout

\begin_layout Standard
The second one outputs four real numbers for each of the 
\begin_inset Formula $K$
\end_inset

 object classes.
 This set of 4 values encodes the final bounding box positions for one of
 the objects from the 
\begin_inset Formula $K$
\end_inset

 classes.
\end_layout

\end_deeper
\begin_layout Standard
In this case, both Fast-RCNN detector and MCG object proposal extract are
 implemented within external Matlab libraries and so, should be computed
 offline and introduced externally to the application.
\end_layout

\begin_layout Subsection
PSP-Net
\end_layout

\begin_layout Standard
As we explained in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Contextual-Model-Generation"

\end_inset

 PSP-Net has been trained to detect people as a class.
 In our proposed system we also use this approach by encapsulating connected
 components and pixels labeled as pedestrians.
\end_layout

\begin_layout Section
Fusion and Filtering
\end_layout

\begin_layout Standard
Once the semantic model and pedestrian detections are obtained the next
 step is to combine the information from different camera sources and project
 detections onto a common plane.
 We call this process fusion and filtering.
\end_layout

\begin_layout Subsection
Multi-camera Scenario
\end_layout

\begin_layout Standard
Our system runs on a multi-camera scenario, hence, the scene can be observed
 from different point of views.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration"

\end_inset

 we can observe how the scene is configured with 3 static video-surveillance
 cameras.
 Two of them are placed at the sides of the scene, while one is at the bottom
 part.
 The starting views from the three static cameras are displayed in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Configuration Static.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Analyzing a multi-camera scenario has some advantages.
 One can see in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 we can observe the same scene area from three different points of views,
 which means that for instance, detections from one camera can be used to
 detect in other camera or even refine the available detections.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 1 Initial View.jpg
	lyxscale 50
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 2 Initial View.jpg
	lyxscale 50
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera 3 Initial View.jpg
	lyxscale 50
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Initial Camera Views
\begin_inset CommandInset label
LatexCommand label
name "fig:Initial Camera Views"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Cenital Plane Homography
\end_layout

\begin_layout Standard
Homography calculation is a pivotal task in our work and the main base for
 the fusion of all the different information coming from the three cameras.
 
\end_layout

\begin_layout Standard
The objective is to compute an homography matrix 
\begin_inset Formula $^{\pi_{ref}}H_{F}$
\end_inset

 that relates one camera frame 
\begin_inset Formula $F_{t}$
\end_inset

 at a time 
\begin_inset Formula $t$
\end_inset

, to a so called cenital plane 
\begin_inset Formula $\pi_{ref}$
\end_inset

, i.e.
 a bird-eye representation of the scene.
 
\end_layout

\begin_layout Standard
With this homography matrix we are able to transform every frame pixel 
\begin_inset Formula $F_{t}(x_{1},\,y_{1})$
\end_inset

 to its correspondent position in the cenital plane 
\begin_inset Formula $\pi_{ref}(x_{2},\,y_{2})$
\end_inset

, i.e.
 from 
\begin_inset Formula $2D\rightarrow\text{3D}$
\end_inset

.
 This process is done by Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Homography Equation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P_{i}=\lambda\cdot{}^{\pi_{ref}}H_{F}\cdot p_{i}\label{eq:Homography Equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\lambda$
\end_inset

 is a scale factor, 
\begin_inset Formula $P_{i}$
\end_inset

 is the subset of points from 
\begin_inset Formula $\pi_{ref}$
\end_inset

 and 
\begin_inset Formula $p_{i}$
\end_inset

 is the subset of points of 
\begin_inset Formula $F$
\end_inset

.
\end_layout

\begin_layout Standard
Frame perspective is so, transformed as if it were being viewed from the
 top.
 The homography process is also important as it enables to project pedestrians
 detections to the cenital plane with the same proceeding.
\end_layout

\begin_layout Standard
In order to compute an homography, a relation between at least 4 points
 in each of the two images should be computed.
 In this case a relationship between a real camera frame and a cenital view
 plane created by computer is needed.
 This means that there is not a real correspondence for pixels and so it
 is not possible to use a point descriptor to extract common points in both
 images.
 User has to manually select the points that represent the same spatial
 place in both images using the Graphical User Interface and then the algorithm
 computes the transformation matrix.
\end_layout

\begin_layout Subsubsection
Cenital Plan Design
\end_layout

\begin_layout Standard
In order to share detections between cameras a common plane is needed.
 For the proposed system the plane depict in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cenital plane with camera positions."

\end_inset

 has been used.
 The complete creation of the plane is detailed in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Cenital Plane Design Append"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Second Approach Cenital View.png
	lyxscale 30
	width 75text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cenital plane with camera positions
\begin_inset CommandInset label
LatexCommand label
name "fig:Cenital plane with camera positions."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This plane, due to its high amount of scene details allows the user to correctly
 select common points between it and a camera frame for the homography calculati
on.
\end_layout

\begin_layout Subsubsection
Camera Panning
\end_layout

\begin_layout Standard
When cameras are static and pointing to the center of the scene as in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Initial Camera Views"

\end_inset

 one can easily observe that the vision range compared to the hole scenario
 representation is quite limited due to the low vision range of the cameras.
 The cameras are covering the middle part leaving completely unattended
 the lateral parts of the scenario.
 This approach is a limitation to the project objectives as high percentage
 of the scene semantic remains hidden.
 
\end_layout

\begin_layout Standard
The solution for this problem is to include panning movement thanks to the
 PTZ technology in all the cameras from left to right so cameras view range
 is increased and they are no longer focus only in the middle of the scene
 as before.
 This leads also to more common and not static areas which the cameras will
 share.
 This solution is presented in the scene diagram in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-camera-configuration with panning set up"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Camera Panning Configuration.png
	lyxscale 30
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Multi-camera configuration with panning setup
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-camera-configuration with panning set up"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Camera Panning Discretization
\end_layout

\begin_layout Standard
Before including camera panning in the set up, homographies where calculated,
 at the beginning, for only one video frame per camera.
 This means that every camera only had one homography matrix as all its
 sequence was static and all its video frames where projected using the
 same matrix.
 
\end_layout

\begin_layout Standard
However, when including video panning this simple approach is no longer
 valid as the scene view is constantly changing.
 The ideal solution for this problem would be having one homography matrix
 
\begin_inset Formula $^{\pi_{ref}}H_{Frame}$
\end_inset

 per video frame.
 As the homographies are calculated by the user selecting the points this
 solution is evidentially impossible in terms of usability.
 
\end_layout

\begin_layout Standard
We propose discretization of the panning tour in which 
\begin_inset Formula $N$
\end_inset

 views are selected from the video sequence.
 This leads to the obtention of an homography codebook in where the user
 computes a homography matrix 
\begin_inset Formula $^{\pi_{ref}}H_{View}$
\end_inset

 for each of the 
\begin_inset Formula $N$
\end_inset

 camera views.
 This codebook of views and homographies are responsible for projecting
 into the cenital plane the hole video.
\end_layout

\begin_layout Subsubsection
View Selection
\begin_inset CommandInset label
LatexCommand label
name "subsec:View-Selection"

\end_inset


\end_layout

\begin_layout Standard
Due to the creation of the homography codebook we now have to choose between
 a set of 
\begin_inset Formula $N$
\end_inset

 homography matrices to project detections into the cenital plane.
 The actual analyzed frame should be compared to each of the 
\begin_inset Formula $N$
\end_inset

 views to obtain an spatial correspondence and so, use the correct homography
 matrix.
\end_layout

\begin_layout Subsubsection*
Inter-image Comparison
\end_layout

\begin_layout Standard
In order to compare two images we have done comparison between points of
 interest.
 AKAZE detector and descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "alcantarilla2011fast"

\end_inset

 has been used for this task (AKAZE is explained in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:AKAZE append"

\end_inset

).
\end_layout

\begin_layout Standard
AKAZE detects and describes points of interest in any image and then by
 a brute force comparator in terms of point distances we extract how many
 coincidence we have between a pair of images.
 
\end_layout

\begin_layout Standard
It is essential to say, that, as we have to compare the current frame with
 all of the 
\begin_inset Formula $N$
\end_inset

 views there is a trade off.
 More views means a better space discretization and more overlapping between
 them, however, the computational time increases exponentially, this means
 that we have to choose the number of views 
\begin_inset Formula $N$
\end_inset

 so it represents correctly the space and keeps the computational time relativel
y low.
 
\end_layout

\begin_layout Standard
In our proposed system we have choose 
\begin_inset Formula $N=9$
\end_inset

 to maintain a trade off between projection quality due to the overlap between
 the views and time consumption.
\end_layout

\begin_layout Subsubsection*
Intermediate Homography
\end_layout

\begin_layout Standard
As we said before, the homographies are calculated for each of the views,
 this means, that if a frame is not positioned exactly as a view the homography
 matrix does not project the points correctly, there will be a small error.
 
\end_layout

\begin_layout Standard
Taking advantage of the calculation of common descriptors between the frame
 and its correspondent view this problem can be solved by calculating automatica
lly the homography between both images.
 This means that now, to project detections from the current frame to the
 cenital plane two homographies are used.
 First we change the perspective of the original frame to the perspective
 of the view with 
\begin_inset Formula $^{View}H_{Frame}$
\end_inset

.
 This process ensures that the used perspective is the same one as the one
 that was used previously to compute the homography.
 
\end_layout

\begin_layout Standard
Finally, the frame and its new perspective are projected to the cenital
 plane by means of 
\begin_inset Formula $^{\pi_{ref}}H_{View}$
\end_inset

.
 This process is graphically explained in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:View selection process and homography between views computation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/View Selection Process.png
	lyxscale 10
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
View selection process and homography calculation
\end_layout

\end_inset

View selection process and homography between views computation.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:View selection process and homography between views computation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Video Sequence Synchronization
\end_layout

\begin_layout Standard
One of the main issues when dealing with multi camera systems, and specially,
 those that combine information between the cameras, is that video sequences
 coming from them should be correctly synchronized.
 This is really important because if the same frame number 
\begin_inset Formula $F$
\end_inset

 is not representing the same exact moment in time in the three cameras,
 combination and fusion of the information is not possible.
\end_layout

\begin_layout Standard
We have synchronized the processed videos using the so called clapperboard
 technique.
 This technique aims to synchronize using concrete events that can be observed
 from all the cameras at the same time.
 By this, we are able to set a synchronize starting point from where the
 following video frames are correctly representing the same moment in time.
\end_layout

\begin_layout Subsection
Pedestrian Filtering and Constraining
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-Filtering-and"

\end_inset


\end_layout

\begin_layout Standard
During this Section we explain how the pedestrian detections are semantic
 filtered and also how the information between people from the three cameras
 is fusion in the system to increase the general algorithm performance.
\end_layout

\begin_layout Subsubsection
Cylinder Estimation
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "miguelez2016Deteccion"

\end_inset

 a cylinder estimation technique is proposed.
 The detected bounding boxes on the camera frame do not correspond spatially
 with the exact position of the detected object due to the camera perspective.
 If this detection error is not corrected when the bounding boxes are projected
 either to another camera instance or to the common cenital plane there
 is a distance between the represented bounding box and the real object.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for camera instances projections"

\end_inset

 shows the case for bounding box transference between cameras.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cylinder estimation between cameras.png
	lyxscale 20
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Cylinder estimation for camera instance projections
\end_layout

\end_inset

Cylinder estimation for camera instance projections.
 Extracted from 
\begin_inset CommandInset citation
LatexCommand cite
key "miguelez2016Deteccion"

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for camera instances projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can observe in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for camera instances projections"

\end_inset

 when the blue bounding boxes are projected from image (a) to image (c)
 they are not correctly on the pedestrian.
 The solution is to compute the cylinder that embraces the square whose
 side is the bounding box (b).
 Once the cylinder is estimated, the person will hypothetically be in the
 middle of the cylinder.
 In (c) we can observe that the estimation, if correctly computed has great
 accuracy.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cylinder-estimation-for cenital view projections"

\end_inset

 the same method is applied but in this case, the bounding box is not projected
 to another camera perspective but to the cenital plane.
 The projection of the bounding box, represented by the green line is not
 in the same position as the center of the cylinder, which is at the end
 of the purple line.
 The center of the cylinder so, corresponds to a more approximate position
 of the detected person.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Cilynder Cenital.png
	lyxscale 20
	width 85text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cylinder estimation for cenital view projections
\begin_inset CommandInset label
LatexCommand label
name "fig:Cylinder-estimation-for cenital view projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Gaussian Representation of Bounding Boxes
\end_layout

\begin_layout Standard
Using also the cylinder representation one can change the way bounding boxes
 are represented in the cenital plane.
 As said before, in the simple cylinder representation pedestrian are represente
d with two perpendicular lines, however, in this representation one does
 have spatial information about the detection but none about the detection
 accuracy.
 This means that with the two perpendicular lines one can observed where
 the detection has been achieved but not the score associated with this
 detections.
 To solve this issue we propose a new representation method based on a Gaussian
 function placed at the middle of the cylinder.
\end_layout

\begin_layout Standard
Every pedestrian detection is so, represented as a Gaussian function of
 the form described in Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Gaussian Function Equation"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f(x,y)=Aexp\left(-\left(\frac{(x-x_{0}){}^{2}}{2\sigma_{x}^{2}}+\frac{(y-y_{0}){}^{2}}{2\sigma_{y}^{2}}\right)\right)\label{eq:Gaussian Function Equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case 
\begin_inset Formula $A$
\end_inset

 is the amplitude which typically is set to 
\begin_inset Formula $A=1$
\end_inset

, 
\begin_inset Formula $x_{0},\:y_{0}$
\end_inset

 represent the mean of the gaussian which, in our case, is the center of
 the cylinder associated to the bounding box and 
\begin_inset Formula $\sigma_{x},\sigma_{y}$
\end_inset

 which is the standard deviation and in our case represents the accuracy
 of the detection.
 
\end_layout

\begin_layout Standard
In order to relate a detection score 
\begin_inset Formula $S_{n}$
\end_inset

 with a determinate standard deviation a simple rule explained in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Score2Deviation"

\end_inset

 is proposed.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sigma_{x}=\sigma_{y}=(S_{n}\cdot10)+5\label{eq:Score2Deviation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This Equation has been adapted to the size of the used cenital plane.
 This means that detections with higher scores are represented as a narrow
 gaussian function (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:High-score-detections"

\end_inset

) while lower scores lead to wide gaussians that represent a bigger area
 in where one can find that person (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Low-score-detections"

\end_inset

).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Gaussian Representation 2.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
High score detection.
 Frame 86
\begin_inset CommandInset label
LatexCommand label
name "fig:High-score-detections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Gaussian Representation 1.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Low score detections.
 Frame 87
\begin_inset CommandInset label
LatexCommand label
name "fig:Low-score-detections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Gaussian representation examples
\begin_inset CommandInset label
LatexCommand label
name "fig:Gaussian-representation-examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Pedestrian Reprojection Between Cameras
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-Reprojection between cameras"

\end_inset


\end_layout

\begin_layout Standard
When all the pedestrian detections have been projected into the cenital
 plane one can start reprojecting those projections from the cenital plane
 back to the other camera frames.
 
\end_layout

\begin_layout Standard
This process has a fundamental importance as it is from where the pedestrian
 detector accuracy can be increase by the use of the multi-camera system.
 For instance, detections from Cameras 2 and 3 are reprojected into Camera
 1.
 Sometimes, those reprojections are not in the frame because the cameras
 are not aiming to the same spatial area, however, when they are seeing
 the same spatial space cameras share those detections.
 Ideally, if the three cameras detect the person reprojection could not
 be necessary, but, if one of them misses the detection, the other two detection
s, by the use of reprojection, lead to suppress that miss detection.
 
\end_layout

\begin_layout Standard
Projected frames from one camera to another are treated as another detector
 blob.
 This means that if the detection is accurate enough in the original camera,
 when projected to another frame it could be either joined by a Non-Maximum
 Suppression (NMS), if that camera has already one blob, or maintained as
 a detection on the person.
 However, if the detection is not accurate enough, the projection may not
 be on the person and so, that blob leads to a false positive.
 This is the main drawback of the method.
\end_layout

\begin_layout Standard
In addition, when blobs are projected from one frame to another blob height
 is lost due to the cenital projection.
 In order to achieve a complete blob once it has been reprojected, aspect
 ratio is used to, using the width of the blob obtain the height.
\end_layout

\begin_layout Standard
This process can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrain reprojection into other cameras"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Multi Camera Reprojection/Cam 1.png
	lyxscale 20
	width 49text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1 + Reprojections from 2 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Multi Camera Reprojection/Cam 2.png
	lyxscale 20
	width 49text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2 + Reprojections from 1 and 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Multi Camera Reprojection/Cam 3.png
	lyxscale 20
	width 49text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3 + Reprojections from 1 and 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Pedestrian detection reprojection
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrain reprojection into other cameras"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Semantic Constraining of Pedestrian Detections
\begin_inset CommandInset label
LatexCommand label
name "subsec:Pedestrian-Semantic-Constraining"

\end_inset


\end_layout

\begin_layout Standard
Once all the detections have been reprojected into all the cameras one can
 use semantic information in order to filter false positives detections
 in not correct semantic scene areas.
 
\end_layout

\begin_layout Standard
Ideally people are always walking on the floor so the common floor areas
 between pairs of cameras, whose calculation is deeply explained Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Semantic-Fusion-in the Reference Plane"

\end_inset

, are used to constrain the pedestrian detections to this hypothesis.
 This process is done based on the cenital plane builded using the cylinder
 estimation explained before.
 
\end_layout

\begin_layout Standard
The constrain idea is that a bounding box coming from a camera 
\begin_inset Formula $C_{1}$
\end_inset

 is assumed correctly computed if the center of its related cylinder is
 on the common floor between 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{2}$
\end_inset

 and also between 
\begin_inset Formula $C_{1}$
\end_inset

 and 
\begin_inset Formula $C_{3}$
\end_inset

.
 An example of the constraining can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pedestrain semantic constraining"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Semantic Constraining/Cam 1.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Semantic Constraining/Cam 2.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Semantic Constraining/Cam 3.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Pedestrian semantic constraining examples
\end_layout

\end_inset

Pedestrian semantic constraining.
 Blobs not floor semantic area are just displayed by a circle and text label.
\begin_inset CommandInset label
LatexCommand label
name "fig:Pedestrain semantic constraining"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Reference RGB and Semantic Planes
\end_layout

\begin_layout Standard
During this Section our proposed method to create both RGB and semantic
 reference planes combining information from all the cameras positions is
 explained.
 The final objective is to have complete maps in each camera that represent
 the scene.
\end_layout

\begin_layout Subsection
Semantic and RGB Reference Plane Generation
\end_layout

\begin_layout Standard
Following with the process scheme of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:View selection process and homography between views computation"

\end_inset

 one can project any video frame into a cenital perspective.
 By this, a set of projected RGB and semantic frames for each camera is
 obtained.
 Some examples of these images can be observed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGB-and-semantic projected"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame207 RGB.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Projected 
\begin_inset Formula $Frame_{t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame1169 RGB.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB Projected 
\begin_inset Formula $Frame_{t+\Delta t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame207 Semantic.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Projected 
\begin_inset Formula $Frame_{t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Frame1169 Semantic.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Semantic Projected 
\begin_inset Formula $Frame_{t+\Delta t}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RGB and semantic frames projected
\begin_inset CommandInset label
LatexCommand label
name "fig:RGB-and-semantic projected"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once all the video frames are projected a temporal average tis applied through
 a median filter for the three cameras separately.
 By this method a reference plane 
\begin_inset Formula $\pi_{ref-C}$
\end_inset

 for each camera contained in the ground plane is created by joining all
 the projected frames.
 Given a set of pixels 
\begin_inset Formula $P(x_{0},\,y_{0},\,N)$
\end_inset

 from the same spatial position 
\begin_inset Formula $(x_{0},y_{0})$
\end_inset

 from 
\begin_inset Formula $N$
\end_inset

 different projected images one can define median filter as in Eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Median Temporal Average"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(\widetilde{x_{0}},\:\widetilde{y_{0}})=\begin{cases}
P_{(N+1)/2} & if\,N\,is\,odd\\
\frac{1}{2}(P_{N/2}+P_{1+N/2}) & if\,N\,is\,even
\end{cases}\label{eq:Median Temporal Average}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The result coming from the temporal averaging can be observed in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGB Median Average"

\end_inset

 for RBG case and in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Semantic Median Average"

\end_inset

 for semantic information.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB1Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1.
 
\begin_inset Formula $^{RGB}\pi_{ref-1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB2Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2.
 
\begin_inset Formula $^{RGB}\pi_{ref-2}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/RGB3Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3.
\begin_inset Formula $^{RGB}\pi_{ref-3}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
RGB Reference Planes
\end_layout

\end_inset

RGB Reference Planes 
\begin_inset Formula $^{RGB}\pi_{ref-C}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:RGB Median Average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem1Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 1 
\begin_inset Formula $^{Sem}\pi_{ref-1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem2Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 2 
\begin_inset Formula $^{Sem}\pi_{ref-2}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Sem3Median.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Camera 3 
\begin_inset Formula $^{Sem}\pi_{ref-3}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Semantic Median Average
\end_layout

\end_inset

Semantic Median Average 
\begin_inset Formula $^{Sem}\pi_{ref-C}$
\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:Semantic Median Average"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As one can observe smooth semantic and RGB maps from all the hall area are
 created.
 It is important to recall that only if the homographies and the ground
 floor are correctly calculated and projected, the base of for instance
 the columns from all the different cameras, should lay in the same spatial
 point in these images.
\end_layout

\begin_layout Subsection
Semantic Fusion in the Reference Plane
\begin_inset CommandInset label
LatexCommand label
name "subsec:Semantic-Fusion-in the Reference Plane"

\end_inset


\end_layout

\begin_layout Standard
Once the cenital maps have been extracted one can combine them to create
 semantic fusion in the reference plane.
 The main objective is to obtain common areas, i.e.
 pixels with the same label within two or more cameras.
 This method allows us to increment the confidence for those pixels and
 so, we can use this information for example, to constrain pedestrians detection
s.
 As the reference plane is obtained by the homography to the ground plane
 we have only take into account floor pixels to create the common areas,
 as the others have projection errors that lead to fusion errors.
\end_layout

\begin_layout Standard
First, pairs of cameras are combined to create three common semantic areas
 that are shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas between pair of cameras."

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic12.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cam 1 and 2.
 
\begin_inset Formula $^{Sem}\pi_{ref-1}\land{}^{Sem}\pi_{ref-2}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic23.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cam 2 and 3.
 
\begin_inset Formula $^{Sem}\pi_{ref-2}\land{}^{Sem}\pi_{ref-3}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemantic13.png
	lyxscale 20
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cam 3 and 1.
  
\begin_inset Formula $^{Sem}\pi_{ref-3}\land{}^{Sem}\pi_{ref-1}$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Common semantic areas between pair of cameras
\begin_inset CommandInset label
LatexCommand label
name "fig:Common semantic areas between pair of cameras."

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Once pairs of cameras have been combined one can go further and extract
 common areas for the three cameras at the same time.
 Those areas are pixels with the exact same label for the three cameras
 so its probability to be that class is really high.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Common semantic areas for all the cameras"

\end_inset

 represents the common areas.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/CommonSemanticAllCameras.png
	lyxscale 20
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Common semantic areas for all the cameras
\end_layout

\end_inset

Common semantic areas for all the cameras 
\begin_inset Formula $^{Sem}\pi_{ref-1}\land{}^{Sem}\pi_{ref-2}\land{}^{Sem}\pi_{ref-3}$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Common semantic areas for all the cameras"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can observe, due to the multi camera setup and some scene occlusions
 such as scene panels or columns, the common area shared by all the cameras
 represents a small portion of the room.
 This is the reason why we have choose common areas between pairs of cameras
 to work along with pedestrian detection rather than common area between
 all of them.
\end_layout

\begin_layout Section
Statistical Usage Data
\end_layout

\begin_layout Standard
Along this Section we explain the proposed system to extract statistical
 usage data frame by frame given a sequence.
 The aim is to obtain one graph per selected class that measures the amount
 of people at a moment 
\begin_inset Formula $t$
\end_inset

 of time in the determined semantic area during the sequence.
\end_layout

\begin_layout Subsection
Statistical Semantic Map Generation
\end_layout

\begin_layout Standard
In order to extract usage data the first step is to create a projected common
 semantic map.
 For this task, rather than creating common areas between cameras as done
 for the PD constraining, information for all the cameras have been joined
 without strictly being shared.
 This have been done to preserve as much semantic as possible and so, have
 bigger floor or door areas than if we were more restrictive.
\end_layout

\begin_layout Standard
The result from this step is a unique map that represents the hole spatial
 area (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Statistical Semantic Map"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Statistical Map.png
	lyxscale 20
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Proposed System/Statistical Map Legend.png
	lyxscale 10
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Statistical semantic map
\begin_inset CommandInset label
LatexCommand label
name "fig:Statistical Semantic Map"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Usage Curves and Paths
\end_layout

\begin_layout Standard
As one can observe we have decided to only take into account for this process
 floor, doors and chairs, which are the most accurate detections from the
 semantic segmentation.
 Following the same principle as in the previous Section, pedestrians are
 projected on to the statistical semantic map and so one can be able to
 know how many people are in each of the designed areas.
 
\end_layout

\begin_layout Standard
In addition, one can divide Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Second cenital plane approach with camera positions. Anex"

\end_inset

 into different subregions of fixed size and extract in which of them pedestrian
s are more likely to be.
 This process leads to a set of most used paths per subregions.
\end_layout

\end_body
\end_document
